{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f93882a9",
   "metadata": {},
   "source": [
    "# AI-Powered Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e0a96a",
   "metadata": {},
   "source": [
    "## Download the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55542eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/N0CceRlquaf9q85PK759WQ/regression-dataset.csv\n",
    "# ! wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/7J73m6Nsz-vmojwab91gMA/classification-dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be29c5",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc36efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-17 20:43:54.533163: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import sklearn\n",
    "import langchain\n",
    "import openai\n",
    "import langchain_ollama\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain_classic.agents import tool, create_openai_tools_agent, create_tool_calling_agent, AgentExecutor\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff325ac",
   "metadata": {},
   "source": [
    "## Langchain Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac0b5cb",
   "metadata": {},
   "source": [
    "### List CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d0a6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def list_csv_files() -> Optional[List[str]]:\n",
    "    \"\"\"List all CSV file names in the local directory.\n",
    "\n",
    "    Returns:\n",
    "        A list containing CSV file names.\n",
    "        If no CSV files are found, returns None.\n",
    "    \"\"\"\n",
    "    csv_files = glob.glob(os.path.join(os.getcwd(), \"*.csv\"))\n",
    "    if not csv_files:\n",
    "        return None\n",
    "    return [os.path.basename(file) for file in csv_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e3938c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Name: list_csv_files\n",
      "Tool Description: List all CSV file names in the local directory.\n",
      "\n",
      "    Returns:\n",
      "        A list containing CSV file names.\n",
      "        If no CSV files are found, returns None.\n",
      "Tool Arguments: {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['classification-dataset.csv', 'regression-dataset.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Tool Name:\", list_csv_files.name)\n",
    "print(\"Tool Description:\", list_csv_files.description)\n",
    "print(\"Tool Arguments:\", list_csv_files.args)\n",
    "\n",
    "files = list_csv_files.invoke({})\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8016e134",
   "metadata": {},
   "source": [
    "### Dataset caching tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3340ec3",
   "metadata": {},
   "source": [
    "While loading datasets multiple times, we waste tokens and context window space. To fix this, create a **global** cache that stores DataFrames after theyâ€™re loaded the first time. This approach has several benefits:\n",
    "\n",
    "1. It reduces token usage by letting you reference datasets by name instead of pasting their contents  \n",
    "2. It improves performance by loading each dataset only once  \n",
    "3. It keeps datasets available across different tool calls  \n",
    "\n",
    "The following tool lets the agent preload datasets into this cache system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1405e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFRAME_CACHE = {}\n",
    "\n",
    "@tool\n",
    "def preload_datasets(paths: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Loads CSV files into a global cache if not already loaded.\n",
    "    \n",
    "    This function helps to efficiently manage datasets by loading them once\n",
    "    and storing them in memory for future use. Without caching, you would\n",
    "    waste tokens describing dataset contents repeatedly in agent responses.\n",
    "    \n",
    "    Args:\n",
    "        paths: A list of file paths to CSV files.\n",
    "\n",
    "    Returns:\n",
    "        A message summarizing which datasets were loaded or already cached.\n",
    "    \"\"\"\n",
    "    loaded = []\n",
    "    cached = []\n",
    "    for path in paths:\n",
    "        if path not in DATAFRAME_CACHE:\n",
    "            DATAFRAME_CACHE[path] = pd.read_csv(path)\n",
    "            loaded.append(path)\n",
    "        else:\n",
    "            cached.append(path)\n",
    "    \n",
    "    return (\n",
    "        f\"Loaded datasets: {loaded}\\n\"\n",
    "        f\"Already cached: {cached}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbaae4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Name: preload_datasets\n",
      "Tool Description: Loads CSV files into a global cache if not already loaded.\n",
      "\n",
      "This function helps to efficiently manage datasets by loading them once\n",
      "and storing them in memory for future use. Without caching, you would\n",
      "waste tokens describing dataset contents repeatedly in agent responses.\n",
      "\n",
      "Args:\n",
      "    paths: A list of file paths to CSV files.\n",
      "\n",
      "Returns:\n",
      "    A message summarizing which datasets were loaded or already cached.\n",
      "Tool Arguments: {'paths': {'items': {'type': 'string'}, 'title': 'Paths', 'type': 'array'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Tool Name:\", preload_datasets.name)\n",
    "print(\"Tool Description:\", preload_datasets.description)\n",
    "print(\"Tool Arguments:\", preload_datasets.args)\n",
    "\n",
    "\n",
    "preload_datasets.invoke({\"paths\":files})\n",
    "\n",
    "len(DATAFRAME_CACHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31868868",
   "metadata": {},
   "source": [
    "### Summarization tool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5543046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional,Dict,Any\n",
    "\n",
    "@tool\n",
    "def get_dataset_summaries(dataset_paths: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Analyze multiple CSV files and return metadata summaries for each.\n",
    "\n",
    "    Args:\n",
    "        dataset_paths (List[str]): \n",
    "            A list of file paths to CSV datasets.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: \n",
    "            A list of summaries, one per dataset, each containing:\n",
    "            - \"file_name\": The path of the dataset file.\n",
    "            - \"column_names\": A list of column names in the dataset.\n",
    "            - \"data_types\": A dictionary mapping column names to their data types (as strings).\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "\n",
    "    for path in dataset_paths:\n",
    "        # Load and cache the dataset if not already cached\n",
    "        if path not in DATAFRAME_CACHE:\n",
    "            DATAFRAME_CACHE[path] = pd.read_csv(path)\n",
    "        \n",
    "        df = DATAFRAME_CACHE[path]\n",
    "\n",
    "        # Build summary\n",
    "        summary = {\n",
    "            \"file_name\": path,\n",
    "            \"column_names\": df.columns.tolist(),\n",
    "            \"data_types\": df.dtypes.astype(str).to_dict()\n",
    "        }\n",
    "\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "523f38bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Name: get_dataset_summaries\n",
      "Tool Description: Analyze multiple CSV files and return metadata summaries for each.\n",
      "\n",
      "Args:\n",
      "    dataset_paths (List[str]): \n",
      "        A list of file paths to CSV datasets.\n",
      "\n",
      "Returns:\n",
      "    List[Dict[str, Any]]: \n",
      "        A list of summaries, one per dataset, each containing:\n",
      "        - \"file_name\": The path of the dataset file.\n",
      "        - \"column_names\": A list of column names in the dataset.\n",
      "        - \"data_types\": A dictionary mapping column names to their data types (as strings).\n",
      "Tool Arguments: {'dataset_paths': {'items': {'type': 'string'}, 'title': 'Dataset Paths', 'type': 'array'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'file_name': 'classification-dataset.csv',\n",
       "  'column_names': ['mean radius',\n",
       "   'mean texture',\n",
       "   'mean perimeter',\n",
       "   'mean area',\n",
       "   'mean smoothness',\n",
       "   'mean compactness',\n",
       "   'mean concavity',\n",
       "   'mean concave points',\n",
       "   'mean symmetry',\n",
       "   'mean fractal dimension',\n",
       "   'radius error',\n",
       "   'texture error',\n",
       "   'perimeter error',\n",
       "   'area error',\n",
       "   'smoothness error',\n",
       "   'compactness error',\n",
       "   'concavity error',\n",
       "   'concave points error',\n",
       "   'symmetry error',\n",
       "   'fractal dimension error',\n",
       "   'worst radius',\n",
       "   'worst texture',\n",
       "   'worst perimeter',\n",
       "   'worst area',\n",
       "   'worst smoothness',\n",
       "   'worst compactness',\n",
       "   'worst concavity',\n",
       "   'worst concave points',\n",
       "   'worst symmetry',\n",
       "   'worst fractal dimension',\n",
       "   'target'],\n",
       "  'data_types': {'mean radius': 'float64',\n",
       "   'mean texture': 'float64',\n",
       "   'mean perimeter': 'float64',\n",
       "   'mean area': 'float64',\n",
       "   'mean smoothness': 'float64',\n",
       "   'mean compactness': 'float64',\n",
       "   'mean concavity': 'float64',\n",
       "   'mean concave points': 'float64',\n",
       "   'mean symmetry': 'float64',\n",
       "   'mean fractal dimension': 'float64',\n",
       "   'radius error': 'float64',\n",
       "   'texture error': 'float64',\n",
       "   'perimeter error': 'float64',\n",
       "   'area error': 'float64',\n",
       "   'smoothness error': 'float64',\n",
       "   'compactness error': 'float64',\n",
       "   'concavity error': 'float64',\n",
       "   'concave points error': 'float64',\n",
       "   'symmetry error': 'float64',\n",
       "   'fractal dimension error': 'float64',\n",
       "   'worst radius': 'float64',\n",
       "   'worst texture': 'float64',\n",
       "   'worst perimeter': 'float64',\n",
       "   'worst area': 'float64',\n",
       "   'worst smoothness': 'float64',\n",
       "   'worst compactness': 'float64',\n",
       "   'worst concavity': 'float64',\n",
       "   'worst concave points': 'float64',\n",
       "   'worst symmetry': 'float64',\n",
       "   'worst fractal dimension': 'float64',\n",
       "   'target': 'int64'}},\n",
       " {'file_name': 'regression-dataset.csv',\n",
       "  'column_names': ['Unnamed: 0',\n",
       "   'MedInc',\n",
       "   'HouseAge',\n",
       "   'AveRooms',\n",
       "   'AveBedrms',\n",
       "   'Population',\n",
       "   'AveOccup',\n",
       "   'Latitude',\n",
       "   'Longitude',\n",
       "   'target'],\n",
       "  'data_types': {'Unnamed: 0': 'int64',\n",
       "   'MedInc': 'float64',\n",
       "   'HouseAge': 'float64',\n",
       "   'AveRooms': 'float64',\n",
       "   'AveBedrms': 'float64',\n",
       "   'Population': 'float64',\n",
       "   'AveOccup': 'float64',\n",
       "   'Latitude': 'float64',\n",
       "   'Longitude': 'float64',\n",
       "   'target': 'float64'}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Tool Name:\", get_dataset_summaries.name)\n",
    "print(\"Tool Description:\", get_dataset_summaries.description)\n",
    "print(\"Tool Arguments:\", get_dataset_summaries.args)\n",
    "\n",
    "\n",
    "summaries =get_dataset_summaries.invoke({\"dataset_paths\":files})\n",
    "\n",
    "summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80faef61",
   "metadata": {},
   "source": [
    "### DataFrame method execution tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a816811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def call_dataframe_method(file_name: str, method: str) -> str:\n",
    "   \"\"\"\n",
    "   Execute a method on a DataFrame and return the result.\n",
    "   This tool lets you run simple DataFrame methods like 'head', 'tail', or 'describe' \n",
    "   on a dataset that has already been loaded and cached using 'preload_datasets'.\n",
    "   Args:\n",
    "       file_name (str): The path or name of the dataset in the global cache.\n",
    "       method (str): The name of the method to call on the DataFrame. Only no-argument \n",
    "                     methods are supported (e.g., 'head', 'describe', 'info').\n",
    "   Returns:\n",
    "       str: The output of the method as a formatted string, or an error message if \n",
    "            the dataset is not found or the method is invalid.\n",
    "   Example:\n",
    "       call_dataframe_method(file_name=\"data.csv\", method=\"head\")\n",
    "   \"\"\"\n",
    "   # Try to get the DataFrame from cache, or load it if not already cached\n",
    "   if file_name not in DATAFRAME_CACHE:\n",
    "       try:\n",
    "           DATAFRAME_CACHE[file_name] = pd.read_csv(file_name)\n",
    "       except FileNotFoundError:\n",
    "           return f\"DataFrame '{file_name}' not found in cache or on disk.\"\n",
    "       except Exception as e:\n",
    "           return f\"Error loading '{file_name}': {str(e)}\"\n",
    "   \n",
    "   df = DATAFRAME_CACHE[file_name]\n",
    "   func = getattr(df, method, None)\n",
    "   if not callable(func):\n",
    "       return f\"'{method}' is not a valid method of DataFrame.\"\n",
    "   try:\n",
    "       result = func()\n",
    "       return str(result)\n",
    "   except Exception as e:\n",
    "       return f\"Error calling '{method}' on '{file_name}': {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d8674b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Name: call_dataframe_method\n",
      "Tool Description: Execute a method on a DataFrame and return the result.\n",
      "This tool lets you run simple DataFrame methods like 'head', 'tail', or 'describe' \n",
      "on a dataset that has already been loaded and cached using 'preload_datasets'.\n",
      "Args:\n",
      "    file_name (str): The path or name of the dataset in the global cache.\n",
      "    method (str): The name of the method to call on the DataFrame. Only no-argument \n",
      "                  methods are supported (e.g., 'head', 'describe', 'info').\n",
      "Returns:\n",
      "    str: The output of the method as a formatted string, or an error message if \n",
      "         the dataset is not found or the method is invalid.\n",
      "Example:\n",
      "    call_dataframe_method(file_name=\"data.csv\", method=\"head\")\n",
      "Tool Arguments: {'file_name': {'title': 'File Name', 'type': 'string'}, 'method': {'title': 'Method', 'type': 'string'}}\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Unnamed: 0  20640 non-null  int64  \n",
      " 1   MedInc      20640 non-null  float64\n",
      " 2   HouseAge    20640 non-null  float64\n",
      " 3   AveRooms    20640 non-null  float64\n",
      " 4   AveBedrms   20640 non-null  float64\n",
      " 5   Population  20640 non-null  float64\n",
      " 6   AveOccup    20640 non-null  float64\n",
      " 7   Latitude    20640 non-null  float64\n",
      " 8   Longitude   20640 non-null  float64\n",
      " 9   target      20640 non-null  float64\n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 1.6 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Tool Name:\", call_dataframe_method.name)\n",
    "print(\"Tool Description:\", call_dataframe_method.description)\n",
    "print(\"Tool Arguments:\", call_dataframe_method.args)\n",
    "\n",
    "\n",
    "summaries = call_dataframe_method.invoke({\n",
    "    \"file_name\": files[1],\n",
    "    \"method\": \"info\"\n",
    "})\n",
    "\n",
    "print(summaries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1243f2",
   "metadata": {},
   "source": [
    "### Model evaluation tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7195da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_squared_error\n",
    "\n",
    "# Assumes this global cache is shared\n",
    "DATAFRAME_CACHE = {}\n",
    "\n",
    "@tool\n",
    "def evaluate_classification_dataset(file_name: str, target_column: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Train and evaluate a classifier on a dataset using the specified target column.\n",
    "    Args:\n",
    "        file_name (str): The name or path of the dataset stored in DATAFRAME_CACHE.\n",
    "        target_column (str): The name of the column to use as the classification target.\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary with the model's accuracy score.\n",
    "    \"\"\"\n",
    "    # Try to get the DataFrame from cache, or load it if not already cached\n",
    "    if file_name not in DATAFRAME_CACHE:\n",
    "        try:\n",
    "            DATAFRAME_CACHE[file_name] = pd.read_csv(file_name)\n",
    "        except FileNotFoundError:\n",
    "            return {\"error\": f\"DataFrame '{file_name}' not found in cache or on disk.\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error loading '{file_name}': {str(e)}\"}\n",
    "    \n",
    "    df = DATAFRAME_CACHE[file_name]\n",
    "    if target_column not in df.columns:\n",
    "        return {\"error\": f\"Target column '{target_column}' not found in '{file_name}'.\"}\n",
    "    \n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "@tool\n",
    "def evaluate_regression_dataset(file_name: str, target_column: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Train and evaluate a regression model on a dataset using the specified target column.\n",
    "    Args:\n",
    "        file_name (str): The name or path of the dataset stored in DATAFRAME_CACHE.\n",
    "        target_column (str): The name of the column to use as the regression target.\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary with RÂ² score and Mean Squared Error.\n",
    "    \"\"\"\n",
    "    # Try to get the DataFrame from cache, or load it if not already cached\n",
    "    if file_name not in DATAFRAME_CACHE:\n",
    "        try:\n",
    "            DATAFRAME_CACHE[file_name] = pd.read_csv(file_name)\n",
    "        except FileNotFoundError:\n",
    "            return {\"error\": f\"DataFrame '{file_name}' not found in cache or on disk.\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error loading '{file_name}': {str(e)}\"}\n",
    "    \n",
    "    df = DATAFRAME_CACHE[file_name]\n",
    "    if target_column not in df.columns:\n",
    "        return {\"error\": f\"Target column '{target_column}' not found in '{file_name}'.\"}\n",
    "    \n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return {\n",
    "        \"r2_score\": r2,\n",
    "        \"mean_squared_error\": mse\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "295267b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Name: evaluate_classification_dataset\n",
      "Tool Description: Train and evaluate a classifier on a dataset using the specified target column.\n",
      "Args:\n",
      "    file_name (str): The name or path of the dataset stored in DATAFRAME_CACHE.\n",
      "    target_column (str): The name of the column to use as the classification target.\n",
      "Returns:\n",
      "    Dict[str, float]: A dictionary with the model's accuracy score.\n",
      "Tool Arguments: {'file_name': {'title': 'File Name', 'type': 'string'}, 'target_column': {'title': 'Target Column', 'type': 'string'}}\n",
      "Tool Name: evaluate_regression_dataset\n",
      "Tool Description: Train and evaluate a regression model on a dataset using the specified target column.\n",
      "Args:\n",
      "    file_name (str): The name or path of the dataset stored in DATAFRAME_CACHE.\n",
      "    target_column (str): The name of the column to use as the regression target.\n",
      "Returns:\n",
      "    Dict[str, float]: A dictionary with RÂ² score and Mean Squared Error.\n",
      "Tool Arguments: {'file_name': {'title': 'File Name', 'type': 'string'}, 'target_column': {'title': 'Target Column', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Tool Name:\", evaluate_classification_dataset.name)\n",
    "print(\"Tool Description:\", evaluate_classification_dataset.description)\n",
    "print(\"Tool Arguments:\", evaluate_classification_dataset.args)\n",
    "\n",
    "print(\"Tool Name:\", evaluate_regression_dataset.name)\n",
    "print(\"Tool Description:\", evaluate_regression_dataset.description)\n",
    "print(\"Tool Arguments:\", evaluate_regression_dataset.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "054081e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9649122807017544}\n"
     ]
    }
   ],
   "source": [
    "result = evaluate_classification_dataset.invoke({\n",
    "    \"file_name\": files[0],\n",
    "    \"target_column\": \"target\"\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32b6c3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r2_score': 0.8590216180805765, 'mean_squared_error': 0.033118421052631575}\n"
     ]
    }
   ],
   "source": [
    "result = evaluate_regression_dataset.invoke({\n",
    "    \"file_name\": files[0],\n",
    "    \"target_column\": \"target\"\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81aa48c",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "Agents in LangChain are advanced components that enable AI models to decide when and how to use tools dynamically. Instead of relying on predefined scripts, agents analyze user queries and choose the best tools to achieve a goal. The next step is defining your agent, which requires specifying how it should think and behave. You'll use `ChatPromptTemplate.from_messages()` to create a structured prompt with three essential components:\n",
    "\n",
    "1. **System message**: This establishes the agent's identity and primary objective. You define it as a data science assistant whose task is to analyze CSV files and determine whether each dataset is suitable for classification or regression based on its structure. This gives the agent a clear purpose and scope.\n",
    "\n",
    "2. **User input**: The `{input}` placeholder will be replaced with the user's actual query. This allows the agent to respond directly to what the user is asking about.\n",
    "\n",
    "3. **Agent scratchpad**: The `{agent_scratchpad}` placeholder is crucial for tool-calling agents as it provides space for the agent to show its reasoning process and track intermediate steps. This enables the agent to build a chain of thought, call tools sequentially, and use the results from one tool to inform decisions about subsequent tool calls.\n",
    "\n",
    "![agents copy.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/TYkDvBmpmmSXx6TftNpJgw/agents%20copy.png)\n",
    "\n",
    "[Reference article for image](https://medium.com/@Shamimw/understanding-langchain-tools-and-agents-a-guide-to-building-smart-ai-applications-e81d200b3c12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92b08b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"You are a data science assistant. Use the available tools to analyze CSV files. \"\n",
    "     \"Your job is to determine whether each dataset is for classification or regression, based on its structure.\"),\n",
    "\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),  # Fixed: Use MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "    (\"user\", \"{input}\"),  # Order doesn't matter much, but user last before scratchpad\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "463b8ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a data science assistant. Use the available tools to analyze CSV files. \"\n",
    "     \"Your job is to determine whether each dataset is for classification or regression.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d8eca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import init_chat_model\n",
    "\n",
    "# llm = init_chat_model(\"llama3.2:1b\", model_provider=\"ollama\", streaming=False )\n",
    "\n",
    "from langchain_ollama import ChatOllama \n",
    "\n",
    "llm = ChatOllama(\n",
    "    model= \"llama3.1:8b\", \n",
    "    temperature=0  # Low temp for reliable tool calling\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e1b64ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[list_csv_files, preload_datasets, get_dataset_summaries, call_dataframe_method, evaluate_classification_dataset, evaluate_regression_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3a8346",
   "metadata": {},
   "source": [
    "### Agent creation and limitations\n",
    "\n",
    "Here, you'll create your agent using `create_openai_tools_agent()`, which combines the language model, tools, and prompt template into a functional agent. However, this raw agent has significant limitations when used directly. It only performs a single step of reasoning and tool usage per invocation, then returns its intermediate thought process rather than a final answer. This behavior occurs because the agent doesn't automatically manage the full loop of thinking, acting, observing results, and continuing to reason until reaching a complete solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "300d54cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the tool calling agent\n",
    "#agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent = create_tool_calling_agent(llm,tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac1783e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke({\n",
    "    \"input\": \"List out the dataset .csv files in current directory\",\n",
    "    \"intermediate_steps\": []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17c35901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Agent decided to call a tool:\n",
      "Tool Name: list_csv_files\n",
      "Tool Input: {}\n",
      "Log:\n",
      " Invoking: `list_csv_files` with `{}`\n"
     ]
    }
   ],
   "source": [
    "# Get the first ToolAgentAction from the list\n",
    "action = response[0]\n",
    "\n",
    "# Print the key details\n",
    "print(\"ðŸ§  Agent decided to call a tool:\")\n",
    "print(\"Tool Name:\", action.tool)\n",
    "print(\"Tool Input:\", action.tool_input)\n",
    "print(\"Log:\\n\", action.log.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79a1b6",
   "metadata": {},
   "source": [
    "### Agent executor ReAct\n",
    "\n",
    "Managing this ReAct loop manually can be cumbersome, which is why you'll use the AgentExecutor. The AgentExecutor wraps the agent and the toolset, and handles the full tool-use loop behind the scenes. It automatically runs the agent, executes the selected tool, takes the result (observation), and feeds it back into the agent until a final answer is reached. Without the executor, you'd have to manually manage every step, including checking whether the agent returned a tool call or a final answer, running the tool, and tracking the intermediate stepsâ€”all of which the executor handles for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc6f6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.agents import AgentExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd304f5b",
   "metadata": {},
   "source": [
    "#### Agent executor configuration\n",
    "\n",
    "The `AgentExecutor` line creates a complete, autonomous agent system by wrapping your basic agent with additional functionality. This executor manages the full ReAct loop (reasoning and acting) that allows the agent to make multiple tool calls in sequence until reaching a final answer. Configure it with several important parameters:\n",
    "\n",
    "1. **agent**: The agent to run for creating a plan and determining actions to take at each step of the execution loop.\n",
    "\n",
    "2. **tools**: The valid tools the agent can call.\n",
    "3. **verbose=True**: Enables detailed logging of each step in the agent's thinking and tool-calling process, which is invaluable for debugging and understanding how the agent arrives at its conclusions.\n",
    "\n",
    "4. **handle_parsing_errors=True**: Rather than crashing, the executor will attempt to recover and continue the conversation.\n",
    "\n",
    "The second line, `agent_executor.agent.stream_runnable = False`, disables streaming mode for the agent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b814397",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    return_intermediate_steps=True   # REQUIRED\n",
    ")\n",
    "\n",
    "agent_executor.agent.stream_runnable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2ccf58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f76c9ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `list_csv_files` with `{}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['classification-dataset.csv', 'regression-dataset.csv']\u001b[0m\u001b[32;1m\u001b[1;3mThe available CSV files are `classification-dataset.csv` and `regression-dataset.csv`.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\n",
    "    \"input\": \"List out the dataset .csv files in current directory\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "422c62fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2aad592c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Agent decided to call a tool:\n",
      "Tool Name: list_csv_files\n",
      "Tool Input: {}\n",
      "Log:\n",
      " Invoking: `list_csv_files` with `{}`\n",
      "\n",
      "ðŸ“¤ Tool Output:\n",
      "['classification-dataset.csv', 'regression-dataset.csv']\n"
     ]
    }
   ],
   "source": [
    "action, tool_result = response[\"intermediate_steps\"][0]\n",
    "\n",
    "print(\"ðŸ§  Agent decided to call a tool:\")\n",
    "print(\"Tool Name:\", action.tool)\n",
    "print(\"Tool Input:\", action.tool_input)\n",
    "print(\"Log:\\n\", action.log.strip())\n",
    "\n",
    "print(\"\\nðŸ“¤ Tool Output:\")\n",
    "print(tool_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c04057c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Ask questions about your dataset (type 'exit' to quit):\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `list_csv_files` with `{}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['classification-dataset.csv', 'regression-dataset.csv']\u001b[0m\u001b[32;1m\u001b[1;3mThe available CSV files are `classification-dataset.csv` and `regression-dataset.csv`.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "my Agent: The available CSV files are `classification-dataset.csv` and `regression-dataset.csv`.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_dataset_summaries` with `{'dataset_paths': ['regression-dataset.csv']}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m[{'file_name': 'regression-dataset.csv', 'column_names': ['Unnamed: 0', 'MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude', 'target'], 'data_types': {'Unnamed: 0': 'int64', 'MedInc': 'float64', 'HouseAge': 'float64', 'AveRooms': 'float64', 'AveBedrms': 'float64', 'Population': 'float64', 'AveOccup': 'float64', 'Latitude': 'float64', 'Longitude': 'float64', 'target': 'float64'}}]\u001b[0m\u001b[32;1m\u001b[1;3mThe dataset 'regression-dataset.csv' has 10 columns: 'Unnamed: 0', 'MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude', and 'target'. The data types of these columns are int64, float64, respectively. This dataset is likely for regression as it contains a target variable named 'target' which is also of type float64.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "my Agent: The dataset 'regression-dataset.csv' has 10 columns: 'Unnamed: 0', 'MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude', and 'target'. The data types of these columns are int64, float64, respectively. This dataset is likely for regression as it contains a target variable named 'target' which is also of type float64.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `evaluate_regression_dataset` with `{'file_name': 'data.csv', 'target_column': 'target_value'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m{'error': \"DataFrame 'data.csv' not found in cache or on disk.\"}\u001b[0m\u001b[32;1m\u001b[1;3mIt seems like the dataset is not available. To proceed with regression analysis, please provide the CSV file.\n",
      "\n",
      "Once you have provided the CSV file, I can use it to determine whether the dataset is suitable for regression analysis. \n",
      "\n",
      "If you need help with data preprocessing or feature engineering, feel free to ask!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "my Agent: It seems like the dataset is not available. To proceed with regression analysis, please provide the CSV file.\n",
      "\n",
      "Once you have provided the CSV file, I can use it to determine whether the dataset is suitable for regression analysis. \n",
      "\n",
      "If you need help with data preprocessing or feature engineering, feel free to ask!\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `evaluate_regression_dataset` with `{'file_name': 'regression-dataset.csv', 'target_column': 'target'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m{'r2_score': 0.8166989587918304, 'mean_squared_error': 0.2401993301200906}\u001b[0m\u001b[32;1m\u001b[1;3mBased on the R-squared score of 0.82 and Mean Squared Error (MSE) of 0.24, it appears that this dataset is suitable for regression analysis.\n",
      "\n",
      "The model can be used to predict the 'target' value with a relatively high degree of accuracy. The R-squared score indicates that about 82% of the variance in the target variable is explained by the predictors, and the MSE suggests that the average difference between predicted and actual values is around 0.24 units.\n",
      "\n",
      "Therefore, I would recommend using regression to predict the 'target' value based on this dataset.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "my Agent: Based on the R-squared score of 0.82 and Mean Squared Error (MSE) of 0.24, it appears that this dataset is suitable for regression analysis.\n",
      "\n",
      "The model can be used to predict the 'target' value with a relatively high degree of accuracy. The R-squared score indicates that about 82% of the variance in the target variable is explained by the predictors, and the MSE suggests that the average difference between predicted and actual values is around 0.24 units.\n",
      "\n",
      "Therefore, I would recommend using regression to predict the 'target' value based on this dataset.\n",
      "see ya later\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“Š Ask questions about your dataset (type 'exit' to quit):\")\n",
    "\n",
    "while True:\n",
    "    user_input=input(\" You:\")\n",
    "    if user_input.strip().lower() in ['exit','quit']:\n",
    "        print(\"see ya later\")\n",
    "        break\n",
    "    result=agent_executor.invoke({\"input\":user_input})\n",
    "    print(f\"my Agent: {result['output']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "G-master (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
