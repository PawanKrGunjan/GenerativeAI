{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1e6c36-75e1-4914-93f1-f3b374e15e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AG2 modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from autogen import ConversableAgent, AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager\n",
    "from autogen.llm_config import LLMConfig\n",
    "#from autogen import ConversableAgent, LLMConfig\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"AG2 modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768cc959-867d-4a36-a3ed-619b0dcdd445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Suppress API key format warning\n",
    "logging.getLogger(\"autogen.oai.client\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "657ce696-3ad4-4ebc-8267-aba9dd3cd7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Define the LLM configuration\n",
    "#llm_config = LLMConfig(api_type=\"ollama\", model=\"llama3.2:latest\")\n",
    "\n",
    "\n",
    "# llm_config = {\n",
    "#     \"config_list\": [{\n",
    "#         \"model\": \"sonar-pro\",\n",
    "#         \"api_key\": os.getenv(\"PERPLEXITY_API_KEY\"),\n",
    "#         \"base_url\": \"https://api.perplexity.ai\",  # NO /v1\n",
    "#         \"api_type\": \"openai\",\n",
    "#         \"temperature\": 0.3,\n",
    "#         \"max_tokens\": 1000\n",
    "#     }]\n",
    "# }\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    config_list=[{\n",
    "        \"model\": \"sonar-pro\",\n",
    "        \"api_key\": os.getenv(\"PERPLEXITY_API_KEY\"),\n",
    "        \"base_url\": \"https://api.perplexity.ai\",\n",
    "        \"api_type\": \"openai\",\n",
    "        \"temperature\": 0.3,\n",
    "        \"max_tokens\": 1000\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e49566-dc96-437d-b2b5-8680f156315f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "What is AI?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "**Artificial intelligence (AI)** is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making.[1] AI enables computers and machines to simulate human cognitive abilities, including comprehension, creativity, and autonomy.[3]\n",
      "\n",
      "## How AI Works\n",
      "\n",
      "AI systems process vast amounts of data to uncover patterns and make predictions.[6] They leverage intelligent algorithms—structured sets of rules that allow software to learn from data patterns—and **neural networks**, which are complex systems of interconnected nodes that mimic the human brain's structure to extract meaning from information.[6] This allows AI to understand language, recognize patterns, make decisions, and even predict future outcomes.[6]\n",
      "\n",
      "## Types of AI\n",
      "\n",
      "**Artificial Narrow Intelligence (ANI)** refers to AI systems designed for specific tasks. Most AI in use today falls into this category, performing specialized functions like speech recognition or image identification.[4]\n",
      "\n",
      "**Artificial General Intelligence (AGI)**, also called \"strong AI,\" is a theoretical state where computer systems would achieve or exceed human-level intelligence across a wide range of tasks.[3][4] This level of AI remains theoretical and does not currently exist, though researchers argue it would require major increases in computing power.[3]\n",
      "\n",
      "## Applications\n",
      "\n",
      "AI is prevalent across many industries, including healthcare, finance, transportation, and customer service.[4] Applications range from optical character recognition that extracts text from images, to self-driving cars, to personalized recommendations and automated customer support systems.[3][6][8]\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'After the (optional) system message(s), user or tool message(s) should alternate with assistant message(s).', 'type': 'invalid_message', 'code': 400}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m      9\u001b[39m assistant = ConversableAgent(\n\u001b[32m     10\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     llm_config=llm_config,\n\u001b[32m     12\u001b[39m     system_message=\u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m user_proxy = ConversableAgent(\n\u001b[32m     16\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33muser_proxy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     human_input_mode=\u001b[33m\"\u001b[39m\u001b[33mNEVER\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m     code_execution_config=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     19\u001b[39m     llm_config=llm_config  \u001b[38;5;66;03m# Same config\u001b[39;00m\n\u001b[32m     20\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43muser_proxy\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43massistant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is AI?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1473\u001b[39m, in \u001b[36mConversableAgent.initiate_chat\u001b[39m\u001b[34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[39m\n\u001b[32m   1471\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1472\u001b[39m         msg2send = \u001b[38;5;28mself\u001b[39m.generate_init_message(message, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1473\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1474\u001b[39m summary = \u001b[38;5;28mself\u001b[39m._summarize_chat(\n\u001b[32m   1475\u001b[39m     summary_method,\n\u001b[32m   1476\u001b[39m     summary_args,\n\u001b[32m   1477\u001b[39m     recipient,\n\u001b[32m   1478\u001b[39m     cache=cache,\n\u001b[32m   1479\u001b[39m )\n\u001b[32m   1480\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1126\u001b[39m, in \u001b[36mConversableAgent.send\u001b[39m\u001b[34m(self, message, recipient, request_reply, silent)\u001b[39m\n\u001b[32m   1124\u001b[39m valid = \u001b[38;5;28mself\u001b[39m._append_oai_message(message, recipient, role=\u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m, name=\u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[32m-> \u001b[39m\u001b[32m1126\u001b[39m     \u001b[43mrecipient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1128\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1129\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMessage can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1130\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1236\u001b[39m, in \u001b[36mConversableAgent.receive\u001b[39m\u001b[34m(self, message, sender, request_reply, silent)\u001b[39m\n\u001b[32m   1234\u001b[39m reply = \u001b[38;5;28mself\u001b[39m.generate_reply(messages=\u001b[38;5;28mself\u001b[39m.chat_messages[sender], sender=sender)\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1236\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1126\u001b[39m, in \u001b[36mConversableAgent.send\u001b[39m\u001b[34m(self, message, recipient, request_reply, silent)\u001b[39m\n\u001b[32m   1124\u001b[39m valid = \u001b[38;5;28mself\u001b[39m._append_oai_message(message, recipient, role=\u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m, name=\u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[32m-> \u001b[39m\u001b[32m1126\u001b[39m     \u001b[43mrecipient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1128\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1129\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMessage can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1130\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1234\u001b[39m, in \u001b[36mConversableAgent.receive\u001b[39m\u001b[34m(self, message, sender, request_reply, silent)\u001b[39m\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1233\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1234\u001b[39m reply = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28mself\u001b[39m.send(reply, sender, silent=silent)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:2881\u001b[39m, in \u001b[36mConversableAgent.generate_reply\u001b[39m\u001b[34m(self, messages, sender, exclude)\u001b[39m\n\u001b[32m   2879\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   2880\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._match_trigger(reply_func_tuple[\u001b[33m\"\u001b[39m\u001b[33mtrigger\u001b[39m\u001b[33m\"\u001b[39m], sender):\n\u001b[32m-> \u001b[39m\u001b[32m2881\u001b[39m     final, reply = \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2882\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[32m   2883\u001b[39m         log_event(\n\u001b[32m   2884\u001b[39m             \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2885\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreply_func_executed\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2889\u001b[39m             reply=reply,\n\u001b[32m   2890\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:2162\u001b[39m, in \u001b[36mConversableAgent.generate_oai_reply\u001b[39m\u001b[34m(self, messages, sender, config, **kwargs)\u001b[39m\n\u001b[32m   2159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m processed_messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, {\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mLLM call blocked by safeguard\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m-> \u001b[39m\u001b[32m2162\u001b[39m extracted_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2164\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2165\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2166\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2167\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2169\u001b[39m \u001b[38;5;66;03m# Process LLM response\u001b[39;00m\n\u001b[32m   2170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:2197\u001b[39m, in \u001b[36mConversableAgent._generate_oai_reply_from_client\u001b[39m\u001b[34m(self, llm_client, messages, cache, **kwargs)\u001b[39m\n\u001b[32m   2194\u001b[39m         all_messages.append(message)\n\u001b[32m   2196\u001b[39m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2197\u001b[39m response = \u001b[43mllm_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2201\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2202\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2203\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2204\u001b[39m extracted_response = llm_client.extract_text_or_completion_object(response)[\u001b[32m0\u001b[39m]\n\u001b[32m   2206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/oai/client.py:1263\u001b[39m, in \u001b[36mOpenAIWrapper.create\u001b[39m\u001b[34m(self, **config)\u001b[39m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1262\u001b[39m     request_ts = get_current_ts()\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1264\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1265\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m openai_result.is_successful:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/oai/client.py:684\u001b[39m, in \u001b[36mOpenAIClient.create\u001b[39m\u001b[34m(self, params)\u001b[39m\n\u001b[32m    682\u001b[39m     \u001b[38;5;28mself\u001b[39m._process_reasoning_model_params(params)\n\u001b[32m    683\u001b[39m params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m response = \u001b[43mcreate_or_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[38;5;66;03m# remove the system_message from the response and add it in the prompt at the start.\u001b[39;00m\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_o1:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/oai/client.py:472\u001b[39m, in \u001b[36mOpenAIClient._handle_openai_bad_request_error.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    464\u001b[39m     error_message = (\n\u001b[32m    465\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis error typically occurs when the agent name contains invalid characters, such as spaces or special symbols.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    466\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure that your agent name follows the correct format and doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt include any unsupported characters.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    467\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCheck the agent name and try again.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    468\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mHere is the full BadRequestError from openai:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me.message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    469\u001b[39m     )\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_message)\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/oai/client.py:455\u001b[39m, in \u001b[36mOpenAIClient._handle_openai_bad_request_error.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    454\u001b[39m     kwargs = OpenAIClient._patch_messages_for_deepseek_reasoner(**kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    457\u001b[39m     response_json = e.response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'After the (optional) system message(s), user or tool message(s) should alternate with assistant message(s).', 'type': 'invalid_message', 'code': 400}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from autogen import ConversableAgent, config_list_from_json\n",
    "\n",
    "# Method 1: Direct dict (recommended)\n",
    "\n",
    "# Method 2: config_list_from_json (if using .env/config.json)\n",
    "# config_list = config_list_from_json(\"OAI_CONFIG_LIST\")\n",
    "\n",
    "assistant = ConversableAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are a helpful assistant.\"\n",
    ")\n",
    "\n",
    "user_proxy = ConversableAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    "    llm_config=llm_config  # Same config\n",
    ")\n",
    "\n",
    "user_proxy.initiate_chat(assistant, message=\"What is AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8423e9b-9b71-49eb-918b-d8a76fd5137e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " \n",
    "# Step 2: Define our two agents — a student and a tutor\n",
    "with llm_config:\n",
    "    # Create the student agent (asks questions)\n",
    "    student = ConversableAgent(\n",
    "        name=\"student\",\n",
    "        system_message=\"You are a curious student. You ask clear, specific questions to learn new concepts.\",\n",
    "        human_input_mode=\"NEVER\"  # disables manual input during chat\n",
    "    )\n",
    "\n",
    "    # Create the tutor agent (responds with beginner-friendly answers)\n",
    "    tutor = ConversableAgent(\n",
    "        name=\"tutor\",\n",
    "        system_message=\"You are a helpful tutor who provides clear and concise explanations suitable for a beginner.\",\n",
    "        human_input_mode=\"NEVER\"\n",
    "    )\n",
    "\n",
    "# Step 3: Start a 2-turn conversation initiated by the student\n",
    "chat_result = student.initiate_chat(\n",
    "    recipient=tutor,                                # who the student is talking to\n",
    "    message=\"Can you explain what a neural network is?\",  # the student's question\n",
    "    max_turns=2,                                     # total number of back-and-forth messages\n",
    "    summary_method=\"reflection_with_llm\"            # generate a final summary using LLM\n",
    ")\n",
    "\n",
    "# Step 4: Print the summary of the conversation\n",
    "print(\"\\nFinal Summary:\")\n",
    "print(chat_result.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc25e26-5f4f-43d3-b115-cf5b675379b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a Technical Expert Agent\n",
    "tech_expert = ConversableAgent(\n",
    "    name=\"tech_expert\",\n",
    "    system_message=\"\"\"You are a senior software engineer with expertise in Python, AI, and system design.\n",
    "    Provide technical, detailed explanations with code examples when appropriate.\n",
    "    Always consider best practices and performance implications.\"\"\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\"\n",
    ")\n",
    "\n",
    "# Create a Creative Writer Agent\n",
    "creative_writer = ConversableAgent(\n",
    "    name=\"creative_writer\",\n",
    "    system_message=\"\"\"You are a creative writer and storyteller.\n",
    "    Your responses are engaging, imaginative, and use vivid descriptions.\n",
    "    You excel at making complex topics accessible through stories and analogies.\"\"\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\"\n",
    ")\n",
    "\n",
    "# Create a Business Analyst Agent\n",
    "business_analyst = ConversableAgent(\n",
    "    name=\"business_analyst\",\n",
    "    system_message=\"\"\"You are a business analyst focused on ROI, efficiency, and strategic planning.\n",
    "    Always consider business impact, costs, and practical implementation.\n",
    "    Provide actionable recommendations with clear metrics.\"\"\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\"\n",
    ")\n",
    "\n",
    "agents = [tech_expert, creative_writer, business_analyst]\n",
    "print(\"Specialized agents created!\")\n",
    "for agent in agents:\n",
    "    print(f\"- {agent.name}: {agent.system_message.split('.')[0]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd9c460-51d9-44a5-948f-aebf0b8127ce",
   "metadata": {},
   "source": [
    "## Built-in Agent Types\n",
    "\n",
    "AG2 provides specialized agent classes built on `ConversableAgent` to streamline common workflows such as task-solving, tool use, and user interaction.\n",
    "\n",
    "#### AssistantAgent — Task-solving LLM assistant\n",
    "\n",
    "`AssistantAgent` is a subclass of `ConversableAgent` configured with a default system message tailored for solving tasks using LLMs. It can suggest Python code blocks, offer debugging suggestions, and provide structured responses.\n",
    "\n",
    "- `human_input_mode`: Defaults to `\"NEVER\"` — the assistant operates autonomously.\n",
    "- `code_execution_config`: Defaults to `False` — it does **not execute code** itself.\n",
    "- Designed to work collaboratively with other agents (for example, `UserProxyAgent`) that handle execution.\n",
    "\n",
    "This agent excels at reasoning, planning, and generating code — and expects others to handle the execution layer.\n",
    "\n",
    "#### UserProxyAgent — Executing code on behalf of the user\n",
    "\n",
    "`UserProxyAgent` is a subclass of `ConversableAgent` that acts as a proxy for the human user. It is designed to **execute code**, simulate user decisions, and provide execution-based feedback to other agents like `AssistantAgent`.\n",
    "\n",
    "- `human_input_mode`: Defaults to `\"ALWAYS\"` — prompts the user at every turn.\n",
    "- `llm_config`: Defaults to `False` — no LLM responses unless explicitly configured.\n",
    "- **Code execution is enabled by default.**\n",
    "\n",
    "You can customize its behavior by:\n",
    "- Registering an auto-reply function via `.register_reply()`.\n",
    "- Overriding `.get_human_input()` to change how user input is gathered.\n",
    "- Overriding `.execute_code_blocks()`, `.run_code()`, or `.execute_function()` to control code execution behavior.\n",
    "\n",
    "These two agents are often paired: the `AssistantAgent` writes code, and the `UserProxyAgent` executes it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9d530-64f5-4cb5-a9be-2e1ce16c2885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Requirements:\n",
    "# !pip install matplotlib numpy  # first run without installing the libraries and see if the agent installs the required libraries itself.\n",
    "\n",
    "from autogen import AssistantAgent, UserProxyAgent, LLMConfig\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "\n",
    "# Step 1: Configure the LLM to use (e.g., GPT-4o Mini via OpenAI)\n",
    "#llm_config = LLMConfig(api_type=\"openai\", model=\"gpt-4o-mini\")\n",
    "# llm_config = LLMConfig(\n",
    "#     config_list=[{\n",
    "#         \"model\": \"sonar-pro\",\n",
    "#         \"api_key\": os.getenv(\"PERPLEXITY_API_KEY\"),\n",
    "#         \"base_url\": \"https://api.perplexity.ai\",\n",
    "#         \"api_type\": \"openai\",\n",
    "#         \"temperature\": 0.3,\n",
    "#         \"max_tokens\": 1000\n",
    "#     }]\n",
    "# )\n",
    "# Step 2: Create the assistant agent (code-writing AI)\n",
    "with llm_config:\n",
    "    assistant = AssistantAgent(\n",
    "        name=\"assistant\",\n",
    "        system_message=\"You are a helpful assistant who writes and explains Python code clearly.\"\n",
    "    )\n",
    "\n",
    "# Step 3: Create the user agent that can execute code\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",  # Automatically executes code without human input\n",
    "    max_consecutive_auto_reply=5,  # Ends after 5 response cycles (assistant + user_proxy turns)\n",
    "    code_execution_config={\n",
    "        \"executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\", timeout=30),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Step 4: Start a simple task that leads to code generation and execution\n",
    "chat_result = user_proxy.initiate_chat(\n",
    "    recipient=assistant,\n",
    "    message=\"\"\"Plot a sine wave using matplotlib from -2π to 2π and save the plot as sine_wave.png.\"\"\",\n",
    "    max_turns=4,  # 2 rounds of assistant ↔ user_proxy\n",
    "    summary_method=\"reflection_with_llm\"  # Optional: final LLM-generated summary\n",
    ")\n",
    "\n",
    "# Step 5: Display the generated figure (optional for notebook environments)\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "image_path = \"coding/sine_wave.png\"\n",
    "if os.path.exists(image_path):\n",
    "    display(Image(filename=image_path))\n",
    "else:\n",
    "    print(\"Plot not found. Please check if the assistant saved the file correctly.\")\n",
    "\n",
    "# Step 6: Print summary\n",
    "print(\"\\n Final Summary:\")\n",
    "print(chat_result.summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfd05b4-a0c0-4ce0-9bcb-a710d8af6002",
   "metadata": {},
   "source": [
    "## Human-in-the-Loop\n",
    "\n",
    "AG2 makes integrating human feedback seamless through its human-in-the-loop functionality, allowing AI agents to collaborate with humans during workflows.  \n",
    "\n",
    "This is crucial for:   \n",
    "\n",
    "- **Critical decisions** requiring human judgment\n",
    "- **High-stakes scenarios** with significant consequences\n",
    "- **Regulatory compliance** requiring human oversight\n",
    "- **Quality assurance** and validation\n",
    "\n",
    "You can configure how and when human input is solicited using the `human_input_mode` parameter:\n",
    "\n",
    "- ALWAYS: Requires human input for every response\n",
    "\n",
    "- NEVER: Operates autonomously without human involvement\n",
    "\n",
    "- TERMINATE: Only requests human input to end conversations\n",
    "\n",
    "For convenience, AG2 provides the specialized `UserProxyAgent` class that automatically sets `human_input_mode` to ALWAYS and supports **code execution**. \n",
    "\n",
    "**important note**: Always use code execution functionality with caution and at your own discretion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f104ab84-1624-411d-b1dd-6a777f94c4fc",
   "metadata": {},
   "source": [
    "### Human-in-the-Loop Example: Bug Triage Bot\n",
    "\n",
    "This example demonstrates how to use AG2’s `ConversableAgent` in `human_input_mode=\"ALWAYS\"` to enable **human-in-the-loop workflows**.\n",
    "\n",
    "We simulate a **bug triage assistant** (`triage_bot`) that classifies bug reports as either:\n",
    "- Escalate (for example, critical crash or security issue),\n",
    "- Close (for example, minor cosmetic issue),\n",
    "- Medium priority (default for others).\n",
    "\n",
    "For each classification, the assistant **asks the human agent for confirmation or correction**. This ensures the AI doesn’t act on high-impact decisions without oversight.\n",
    "\n",
    "At the end, the assistant summarizes the triage results.\n",
    "\n",
    "---\n",
    "\n",
    "### Try these inputs when prompted\n",
    "\n",
    "When you’re prompted to reply as the human agent, try responding with the following:\n",
    "\n",
    "- **Confirm assistant’s suggestion**  \n",
    "  `\"Yes, escalate it.\"`  \n",
    "  `\"Closing this makes sense.\"`\n",
    "\n",
    "- **Override assistant’s suggestion**  \n",
    "  `\"This should be marked as high priority instead.\"`  \n",
    "  `\"Let’s keep this open for now.\"`\n",
    "\n",
    "- **Ask for clarification**  \n",
    "  `\"Why do you think this is low priority?\"`  \n",
    "  `\"Can you provide more reasoning?\"`\n",
    "\n",
    "You can also type `exit` at any time to end the conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034e00ce-4724-4c50-b608-262a5a6db8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent, LLMConfig\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "# Step 1: Configure the LLM to use (e.g., GPT-4o Mini via OpenAI)\n",
    "#llm_config = LLMConfig(api_type=\"openai\", model=\"gpt-4o-mini\")\n",
    "# llm_config = LLMConfig(\n",
    "#     config_list=[{\n",
    "#         \"model\": \"sonar-pro\",\n",
    "#         \"api_key\": os.getenv(\"PERPLEXITY_API_KEY\"),\n",
    "#         \"base_url\": \"https://api.perplexity.ai\",\n",
    "#         \"api_type\": \"openai\",\n",
    "#         \"temperature\": 0.3,\n",
    "#         \"max_tokens\": 1000\n",
    "#     }]\n",
    "# )\n",
    "# Step 2: Define system message for bug triage assistant\n",
    "triage_system_message = \"\"\"\n",
    "You are a bug triage assistant. You will be given bug report summaries.\n",
    "\n",
    "For each bug:\n",
    "- If it is urgent (e.g., 'crash', 'security', or 'data loss' is mentioned), escalate it and ask the human agent for confirmation.\n",
    "- If it seems minor (e.g., cosmetic, typo), suggest closing it but still ask for human review.\n",
    "- Otherwise, classify it as medium priority and ask the human for review.\n",
    "\n",
    "Once all bugs are processed, summarize what was escalated, closed, or marked as medium priority.\n",
    "End by saying: \"You can type exit to finish.\"\n",
    "\"\"\"\n",
    "\n",
    "# Step 3: Create the assistant agent\n",
    "with llm_config:\n",
    "    triage_bot = ConversableAgent(\n",
    "        name=\"triage_bot\",\n",
    "        system_message=triage_system_message,\n",
    "    )\n",
    "\n",
    "# Step 4: Create the human agent who will review each recommendation\n",
    "human = ConversableAgent(\n",
    "    name=\"human\",\n",
    "    human_input_mode=\"ALWAYS\",  # prompts for input at each step\n",
    ")\n",
    "\n",
    "# Step 5: Generate sample bug reports\n",
    "BUGS = [\n",
    "    \"App crashes when opening user profile.\",\n",
    "    \"Minor UI misalignment on settings page.\",\n",
    "    \"Password reset email not sent consistently.\",\n",
    "    \"Typo in the About Us footer text.\",\n",
    "    \"Database connection timeout under heavy load.\",\n",
    "    \"Login form allows SQL injection attack.\",\n",
    "]\n",
    "\n",
    "random.shuffle(BUGS)\n",
    "selected_bugs = BUGS[:3]\n",
    "\n",
    "# Format the initial task\n",
    "initial_prompt = (\n",
    "    \"Please triage the following bug reports one by one:\\n\\n\" +\n",
    "    \"\\n\".join([f\"{i+1}. {bug}\" for i, bug in enumerate(selected_bugs)])\n",
    ")\n",
    "\n",
    "# Step 6: Start the conversation\n",
    "response = human.run(\n",
    "    recipient=triage_bot,\n",
    "    message=initial_prompt,\n",
    ")\n",
    "\n",
    "# Step 7: Display the response\n",
    "response.process()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833fd37-aa0c-4df5-a59e-24b2a1dc2e65",
   "metadata": {},
   "source": [
    "## Agent Orchestration & Multi-Agent Systems in AG2<a name=\"agent-orchestration\"></a>\n",
    "\n",
    "AG2 enables the coordination of multiple intelligent agents to collaboratively solve complex tasks. This is known as **agent orchestration** — a powerful design pattern where each agent plays a specialized role, and a **group manager** handles the conversation flow.\n",
    "\n",
    "### Why Multi-Agent Systems?\n",
    "\n",
    "Many real-world problems require more than just a single AI assistant. With AG2, you can:\n",
    "\n",
    "- Assign specific roles and responsibilities to different agents.\n",
    "- Orchestrate conversations using built-in patterns (for example, Auto, RoundRobin, Manual).\n",
    "- Enable agents to build on each other's outputs and refine ideas.\n",
    "- Incorporate human agents for oversight, decisions, or approval.\n",
    "\n",
    "### Orchestration Patterns in AG2\n",
    "\n",
    "AG2 provides several orchestration patterns to structure agent interactions:\n",
    "\n",
    "- **Two-Agent Chat**: Simple back-and-forth between two agents.\n",
    "- **Sequential Chat**: Conversations where one agent’s output becomes another’s input.\n",
    "- **Group Chat**: Multiple agents interact, with selection logic for who speaks next.\n",
    "- **Nested Chat**: Reusable sub-conversations packaged as a single workflow.\n",
    "\n",
    "---\n",
    "\n",
    "## GroupChat and GroupChatManager\n",
    "\n",
    "In AG2, multi-agent collaboration is coordinated using `GroupChat` and `GroupChatManager`.\n",
    "\n",
    "### GroupChat\n",
    "\n",
    "`GroupChat` defines a team of agents and how they interact in a shared conversation. It includes:\n",
    "\n",
    "- **Agents**: A list of AI (or human) agents that participate in the group dialogue.\n",
    "- **Speaker Selection Method**: Determines which agent speaks next. Options include:\n",
    "  - `\"auto\"`: Uses the LLM to select the most contextually appropriate agent.\n",
    "  - `\"round_robin\"`: Agents take turns in a fixed sequence.\n",
    "  - `\"manual\"`: Human selects the next speaker.\n",
    "  - `\"random\"`: Agents are chosen randomly.\n",
    "\n",
    "This structure allows you to define collaborative workflows where agents build on each other’s contributions.\n",
    "\n",
    "### GroupChatManager\n",
    "\n",
    "`GroupChatManager` is responsible for managing the flow of the group conversation and it:\n",
    "\n",
    "- Orchestrates message passing between agents.\n",
    "- Decides when to stop the conversation (for example, based on a termination condition or turn limit).\n",
    "- Leverages the speaker selection method defined in `GroupChat`.\n",
    "\n",
    "It acts like a facilitator that ensures the conversation runs according to the logic you've configured.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. A conversation is initiated by one of the agents (often a user or teacher agent).\n",
    "2. The `GroupChatManager` uses the selected pattern (for example, AutoPattern) to determine which agent speaks next.\n",
    "3. Agents take turns responding based on their roles and system messages.\n",
    "4. The conversation ends either when a stop condition (for example, a message like \"DONE\") is met or when a maximum number of turns is reached.\n",
    "\n",
    "This setup enables modular, role-based collaboration — ideal for use cases like lesson planning, research workflows, or multi-perspective decision-making.\n",
    "\n",
    "To explore more orchestration patterns, visit the AG2 documentation at:  \n",
    "https://docs.ag2.ai/latest/docs/user-guide/advanced-concepts/orchestration/group-chat/introduction/\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Group Chat for Lesson Planning\n",
    "\n",
    "This example shows how a teacher agent collaborates with a planner and a reviewer to create a lesson plan, using AG2’s `GroupChat` and `AutoPattern` to manage the conversation.\n",
    "\n",
    "**Note:** The `is_termination_msg` parameter used for `teacher` agent defines a custom rule to end the conversation —  \n",
    "in this case, the workflow stops when the teacher replies with \"DONE!\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d13914-9668-4b76-84d9-6e4435c930e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent, GroupChat, GroupChatManager, LLMConfig\n",
    "\n",
    "# Replace \"PLACEHOLDER\" with your actual OpenAI API key if running locally\n",
    "#llm_config = LLMConfig(api_type=\"openai\", model=\"gpt-4o-mini\", )  # add api_key=\"PLACE_HOLDER\" Replace with your API key to run outside this learning environment\n",
    "\n",
    "# Define system messages and agent descriptions\n",
    "planner_message = \"Create a short lesson plan for 4th graders.\"\n",
    "reviewer_message = \"Review a plan and suggest up to 3 brief edits.\"\n",
    "teacher_message = \"Suggest a topic and reply DONE when satisfied.\"\n",
    "\n",
    "with llm_config:\n",
    "    lesson_planner = ConversableAgent(\n",
    "        name=\"planner_agent\",\n",
    "        system_message=planner_message,\n",
    "        description=\"Makes lesson plans.\",\n",
    "    )\n",
    "\n",
    "    lesson_reviewer = ConversableAgent(\n",
    "        name=\"reviewer_agent\",\n",
    "        system_message=reviewer_message,\n",
    "        description=\"Reviews lesson plans and suggests edits.\",\n",
    "    )\n",
    "\n",
    "    teacher = ConversableAgent(\n",
    "        name=\"teacher_agent\",\n",
    "        system_message=teacher_message,\n",
    "        is_termination_msg=lambda x: \"DONE\" in (x.get(\"content\", \"\") or \"\").upper()\n",
    "    )\n",
    "\n",
    "# Configure the group chat with automatic speaker selection\n",
    "groupchat = GroupChat(\n",
    "    agents=[teacher, lesson_planner, lesson_reviewer],\n",
    "    speaker_selection_method=\"auto\"  # Uses AutoPattern\n",
    ")\n",
    "\n",
    "manager = GroupChatManager(\n",
    "    name=\"group_manager\",\n",
    "    groupchat=groupchat,\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "# Start with a short initial prompt to keep tokens low\n",
    "teacher.initiate_chat(\n",
    "    recipient=manager,\n",
    "    message=\"Make a simple lesson about the moon.\",\n",
    "    max_turns=6,  # Limit total rounds (e.g., 2 per agent max) -  As a safeguard, it's always best to use max_turns to prevent runaway loops.\n",
    "    summary_method=\"reflection_with_llm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc46a7c-df0b-4013-add1-50497b0e498b",
   "metadata": {},
   "source": [
    "## Tools and Extensions<a name=\"tools\"></a>\n",
    "\n",
    "Tools extend agent capabilities beyond text conversations, allowing them to:\n",
    "- Execute code\n",
    "- Access external APIs\n",
    "- Perform calculations\n",
    "- Interact with databases\n",
    "- Generate visualizations\n",
    "\n",
    "Agents gain significant utility through tools as they provide access to external data, APIs, and functionality.  \n",
    "\n",
    "Let's explore how to integrate tools with AG2 agents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1adae1-426c-4484-85bb-45d6f6fa9402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent, register_function, LLMConfig\n",
    "from typing import Annotated\n",
    "\n",
    "# Replace with your actual key if running outside this environment\n",
    "#llm_config = LLMConfig(api_type=\"openai\", model=\"gpt-4o-mini\")\n",
    "llm_config = LLMConfig(\n",
    "    config_list=[{\n",
    "        \"model\": \"sonar-pro\",\n",
    "        \"api_key\": os.getenv(\"PERPLEXITY_API_KEY\"),\n",
    "        \"base_url\": \"https://api.perplexity.ai\",\n",
    "        \"api_type\": \"openai\",\n",
    "        \"temperature\": 0.3,\n",
    "        \"max_tokens\": 1000\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Define a simple utility function to check if a number is prime\n",
    "def is_prime(n: Annotated[int, \"Positive integer\"]) -> str:\n",
    "    if n < 2:\n",
    "        return \"No\"\n",
    "    for i in range(2, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            return \"No\"\n",
    "    return \"Yes\"\n",
    "\n",
    "# Create the asking agent and the tool-using agent\n",
    "with llm_config:\n",
    "    math_asker = ConversableAgent(\n",
    "        name=\"math_asker\",\n",
    "        system_message=\"Ask whether a number is prime.\"\n",
    "    )\n",
    "    math_checker = ConversableAgent(\n",
    "        name=\"math_checker\",\n",
    "        human_input_mode=\"NEVER\"\n",
    "    )\n",
    "\n",
    "# Register the function between the two agents\n",
    "register_function(\n",
    "    is_prime,\n",
    "    caller=math_asker,\n",
    "    executor=math_checker,\n",
    "    description=\"Check if a number is prime. Returns Yes or No.\"\n",
    ")\n",
    "\n",
    "# Start a brief conversation\n",
    "math_checker.initiate_chat(\n",
    "    recipient=math_asker,\n",
    "    message=\"Is 72 a prime number?\",\n",
    "    max_turns=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77cb4b6-ffcd-441e-b66c-fb8e8ae868b2",
   "metadata": {},
   "source": [
    "### What is `register_function(...)` ?\n",
    "\n",
    "In AG2, `register_function(...)` is used to expose a Python function as a **tool** that can be executed by one agent on behalf of another. This enables agents to delegate tasks like computation, data processing, or external API calls.\n",
    "\n",
    "#### Purpose\n",
    "- Extend agent capabilities beyond text generation.\n",
    "- Allow agents to solve tasks through **function execution**.\n",
    "- Enable collaborative workflows between a **caller** and an **executor** agent.\n",
    "\n",
    "#### Parameters\n",
    "- **function**: A regular Python function to be used as a tool.\n",
    "- **caller**: The agent that will request the tool to be used.\n",
    "- **executor**: The agent that will actually execute the function.\n",
    "- **description** *(optional)*: A natural language description of the function for the LLM to decide when to use it.\n",
    "\n",
    "#### Example\n",
    "```python\n",
    "register_function(\n",
    "    is_prime,\n",
    "    caller=math_asker,\n",
    "    executor=math_checker,\n",
    "    description=\"Check if a number is prime. Returns Yes or No.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd93644-5346-4c9c-9e01-a190a836ca39",
   "metadata": {},
   "source": [
    "## Structured Outputs<a name=\"structured-outputs\"></a>\n",
    "\n",
    "Structured outputs ensure consistent, validated agent responses using Pydantic models.   \n",
    "\n",
    "This is crucial for:\n",
    "  \n",
    "- **Data validation**: Ensuring response format consistency\n",
    "- **API integration**: Reliable data exchange\n",
    "- **Quality assurance**: Preventing malformed outputs\n",
    "- **Type safety**: Clear data contracts\n",
    "\n",
    "**Analogy:** Like standardized hospital forms ensure doctors always record patient info the same way, structured outputs make sure agents respond in predictable, machine-readable formats.\n",
    "\n",
    "\n",
    "In AG2, structured outputs are implemented using Pydantic models and the `response_format` parameter in the `LLMConfig`.\n",
    "\n",
    "To ensure that your agent always returns outputs in a consistent structure, you define a Pydantic class (for example, `ResponseModel`) and assign it to the `response_format` argument of your configuration.\n",
    "\n",
    "This tells the underlying LLM to return a JSON-compatible response matching the defined schema.\n",
    "\n",
    "```python\n",
    "class ResponseModel(BaseModel):\n",
    "    name: str\n",
    "    status: str\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    api_type=\"openai\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    response_format=ResponseModel\n",
    ")\n",
    "```\n",
    "\n",
    "With this setup:\n",
    "\n",
    "- The agent automatically formats its responses to match the ResponseModel. \n",
    "- You don’t need to prompt the LLM to format its response.\n",
    "- The response is parsed and validated by AG2.\n",
    "\n",
    "This approach is essential for reliable automation, integrations, and downstream processing.\n",
    "\n",
    "Let's implement structured outputs with AG2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355cb2a5-d7c0-4933-ad40-211c6d79a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from autogen import ConversableAgent, LLMConfig\n",
    "\n",
    "# Define the structure of the agent's output\n",
    "class TicketSummary(BaseModel):\n",
    "    customer_name: str\n",
    "    issue_type: str\n",
    "    urgency_level: str\n",
    "    recommended_action: str\n",
    "\n",
    "# Configure the LLM with the structured output format\n",
    "# llm_config = LLMConfig(\n",
    "#     api_type=\"openai\",\n",
    "#     model=\"gpt-4o-mini\",\n",
    "#     response_format=TicketSummary,\n",
    "# )\n",
    "llm_config = LLMConfig(\n",
    "    config_list=[{\n",
    "        \"model\": \"sonar-pro\",\n",
    "        \"api_key\": os.getenv(\"PERPLEXITY_API_KEY\"),\n",
    "        \"base_url\": \"https://api.perplexity.ai\",\n",
    "        \"api_type\": \"openai\",\n",
    "        \"temperature\": 0.3,\n",
    "        \"max_tokens\": 1000\n",
    "    }]\n",
    "    response_format=TicketSummary,\n",
    ")\n",
    "\n",
    "# Create the agent\n",
    "with llm_config:\n",
    "    support_agent = ConversableAgent(\n",
    "        name=\"support_agent\",\n",
    "        system_message=(\n",
    "            \"You are a support assistant. Summarize a customer ticket using:\"\n",
    "            \"\\n- customer_name\"\n",
    "            \"\\n- issue_type (e.g. login issue, billing problem, bug report)\"\n",
    "            \"\\n- urgency_level (Low, Medium, High)\"\n",
    "            \"\\n- recommended_action\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# Start a structured conversation\n",
    "support_agent.initiate_chat(\n",
    "    recipient=support_agent,\n",
    "    message=\"Ticket: John Doe is unable to reset his password and has an important meeting in 30 minutes.\",\n",
    "    max_turns=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c604bbdb-640e-49a3-91a8-ed2cb87b6eca",
   "metadata": {},
   "source": [
    "## Best Practices<a name=\"best-practices\"></a>\n",
    "\n",
    "Based on our exploration of AG2 concepts, here are key best practices for building robust agent systems:\n",
    "\n",
    "### Configuration and security\n",
    "- **Never hardcode API keys** - use environment variables\n",
    "- **Use config lists** for production systems with fallback models\n",
    "- **Set appropriate temperature** values (0.0 for deterministic, 0.7-1.0 for creative)\n",
    "- **Implement rate limiting** and error handling\n",
    "\n",
    "### Agent design\n",
    "- **Write clear system messages** that define role, capabilities, and constraints\n",
    "- **Set max_consecutive_auto_reply** to prevent infinite loops\n",
    "- **Choose appropriate human_input_mode** based on use case\n",
    "- **Specialize agents** for specific tasks rather than creating generalists\n",
    "\n",
    "### HITL implementation\n",
    "- **Use HITL for high-stakes decisions** requiring human judgment\n",
    "- **Implement clear escalation criteria** (amount thresholds, risk levels)\n",
    "- **Provide context** to human supervisors for informed decisions\n",
    "- **Log all human interventions** for audit trails\n",
    "\n",
    "### Multi-agent orchestration\n",
    "- **Design clear workflows** with defined handoffs between agents\n",
    "- **Use GroupChat** for collaborative problem-solving\n",
    "- **Implement termination conditions** to prevent endless conversations\n",
    "- **Monitor conversation quality** and intervention points\n",
    "\n",
    "### Tools and integration\n",
    "- **Create focused tools** for specific capabilities\n",
    "- **Implement proper error handling** in tool functions\n",
    "- **Validate tool inputs** and outputs\n",
    "- **Document tool capabilities** clearly for agents\n",
    "\n",
    "### Structured outputs\n",
    "- **Use Pydantic models** for data validation\n",
    "- **Define clear schemas** for consistent outputs\n",
    "- **Implement proper validation** with meaningful error messages\n",
    "- **Version your schemas** for backward compatibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14418971-c6d5-45eb-9fc4-da0dca79b43e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
