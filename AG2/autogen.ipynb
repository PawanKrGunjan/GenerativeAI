{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6177dc-9f53-434c-8795-46a55b38a722",
   "metadata": {},
   "source": [
    "# Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1e6c36-75e1-4914-93f1-f3b374e15e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AG2 modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from autogen import ConversableAgent, AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager\n",
    "from autogen.tools.experimental import PerplexitySearchTool\n",
    "from autogen.llm_config import LLMConfig\n",
    "#from autogen import ConversableAgent, LLMConfig\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"AG2 modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768cc959-867d-4a36-a3ed-619b0dcdd445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Suppress API key format warning\n",
    "logging.getLogger(\"autogen.oai.client\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4ba91d-0b21-4927-8581-281995e14342",
   "metadata": {},
   "source": [
    "### LLM Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "657ce696-3ad4-4ebc-8267-aba9dd3cd7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list=[\n",
    "    {\n",
    "        \"model\": \"sonar-pro\",\n",
    "        \"api_key\": os.getenv(\"PERPLEXITY_API_KEY\"),\n",
    "        \"base_url\": \"https://api.perplexity.ai\",\n",
    "        \"api_type\": \"openai\",\n",
    "        \"temperature\": 0.3,\n",
    "        \"max_tokens\": 1000},\n",
    "    {\n",
    "        \"model\": \"llama3.2:latest\",\n",
    "        \"api_type\": 'ollama',\n",
    "        \"client_host\": \"http://192.168.0.1:11434\",\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 200}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8595dd0d-95c5-43d3-b51e-c187f3c23a61",
   "metadata": {},
   "source": [
    "#### PERPLEXITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abc9f550-c7d6-4e10-bc60-792e52ba58d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMConfig(config_list=[{'max_tokens': 1000, 'temperature': 0.3, 'api_type': 'openai', 'model': 'sonar-pro', 'api_key': '**********', 'base_url': 'https://api.perplexity.ai/', 'tags': [], 'stream': False}])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_config = LLMConfig(config_list[0])\n",
    "perplexity_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877eac9f-7c87-413b-92c5-76688fc769e9",
   "metadata": {},
   "source": [
    "#### OLLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7ab8b82-aa00-40b0-8ca6-e84f1c41f561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMConfig(config_list=[{'max_tokens': 200, 'temperature': 0.0, 'api_type': 'ollama', 'model': 'llama3.2:latest', 'tags': [], 'client_host': HttpUrl('http://192.168.0.1:11434/'), 'stream': False, 'num_predict': -1, 'num_ctx': 2048, 'repeat_penalty': 1.1, 'seed': 0, 'top_k': 40, 'hide_tools': 'never', 'native_tool_calls': False}])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_config = LLMConfig(config_list[1])\n",
    "ollama_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deae29e2-b9bf-4c9c-986f-843d19f98ed8",
   "metadata": {},
   "source": [
    "## Create our LLM agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbb2de9-de2e-4d31-84c9-a095ba5aed8f",
   "metadata": {},
   "source": [
    "### Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa665c25-4e23-480a-8cbe-d822f84afd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = ConversableAgent(\n",
    "    name=\"student\",\n",
    "    llm_config=perplexity_config,\n",
    "    system_message=\"You are a curious student. You ask clear, specific questions to learn new concepts.\",\n",
    "    human_input_mode=\"NEVER\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aa7ec2-aba0-4320-8c3b-5def3a8ac4a7",
   "metadata": {},
   "source": [
    "#### Run the agent with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf5b4b8e-0cc9-4fd7-b066-c281f6605231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to student):\n",
      "\n",
      "Answer Precisely /n What is AI?\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "<html>\r\n<head><title>401 Authorization Required</title></head>\r\n<body>\r\n<center><h1>401 Authorization Required</h1></center>\r\n<hr><center>openresty/1.27.4</center>\r\n<script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML=\"window.__CF$cv$params={r:'9b8bb97289e4ecc5',t:'MTc2NzU0MDA3My4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);\";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script></body>\r\n</html>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m student_response = student.run(\n\u001b[32m      3\u001b[39m     message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnswer Precisely /n \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     max_turns=\u001b[32m1\u001b[39m,\n\u001b[32m      5\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Iterate through the chat automatically with console output\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mstudent_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/io/run_response.py:200\u001b[39m, in \u001b[36mRunResponse.process\u001b[39m\u001b[34m(self, processor)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, processor: EventProcessorProtocol | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    199\u001b[39m     processor = processor \u001b[38;5;129;01mor\u001b[39;00m ConsoleEventProcessor()\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/io/processors/console_event_processor.py:20\u001b[39m, in \u001b[36mConsoleEventProcessor.process\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, response: \u001b[33m\"\u001b[39m\u001b[33mRunResponseProtocol\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevents\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/io/run_response.py:155\u001b[39m, in \u001b[36mRunResponse._queue_generator\u001b[39m\u001b[34m(self, q)\u001b[39m\n\u001b[32m    152\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, ErrorEvent):\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m event.content.error  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m queue.Empty:\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1529\u001b[39m, in \u001b[36mConversableAgent.run.<locals>.initiate_chat\u001b[39m\u001b[34m(self, iostream, response)\u001b[39m\n\u001b[32m   1527\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1528\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m msg_to == \u001b[33m\"\u001b[39m\u001b[33magent\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m         chat_result = \u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1530\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1531\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1532\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclear_history\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclear_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1533\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_turns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_turns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1534\u001b[39m \u001b[43m            \u001b[49m\u001b[43msummary_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43msummary_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1535\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1536\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1537\u001b[39m         chat_result = \u001b[38;5;28mself\u001b[39m.initiate_chat(\n\u001b[32m   1538\u001b[39m             executor,\n\u001b[32m   1539\u001b[39m             message=message,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1542\u001b[39m             summary_method=summary_method,\n\u001b[32m   1543\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1460\u001b[39m, in \u001b[36mConversableAgent.initiate_chat\u001b[39m\u001b[34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[39m\n\u001b[32m   1458\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m msg2send \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1459\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1460\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# No breaks in the for loop, so we have reached max turns\u001b[39;00m\n\u001b[32m   1462\u001b[39m     iostream.send(\n\u001b[32m   1463\u001b[39m         TerminationEvent(\n\u001b[32m   1464\u001b[39m             termination_reason=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMaximum turns (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_turns\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) reached\u001b[39m\u001b[33m\"\u001b[39m, sender=\u001b[38;5;28mself\u001b[39m, recipient=recipient\n\u001b[32m   1465\u001b[39m         )\n\u001b[32m   1466\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1126\u001b[39m, in \u001b[36mConversableAgent.send\u001b[39m\u001b[34m(self, message, recipient, request_reply, silent)\u001b[39m\n\u001b[32m   1124\u001b[39m valid = \u001b[38;5;28mself\u001b[39m._append_oai_message(message, recipient, role=\u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m, name=\u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[32m-> \u001b[39m\u001b[32m1126\u001b[39m     \u001b[43mrecipient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1128\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1129\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMessage can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1130\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:1234\u001b[39m, in \u001b[36mConversableAgent.receive\u001b[39m\u001b[34m(self, message, sender, request_reply, silent)\u001b[39m\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1233\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1234\u001b[39m reply = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28mself\u001b[39m.send(reply, sender, silent=silent)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:2881\u001b[39m, in \u001b[36mConversableAgent.generate_reply\u001b[39m\u001b[34m(self, messages, sender, exclude)\u001b[39m\n\u001b[32m   2879\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   2880\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._match_trigger(reply_func_tuple[\u001b[33m\"\u001b[39m\u001b[33mtrigger\u001b[39m\u001b[33m\"\u001b[39m], sender):\n\u001b[32m-> \u001b[39m\u001b[32m2881\u001b[39m     final, reply = \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2882\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[32m   2883\u001b[39m         log_event(\n\u001b[32m   2884\u001b[39m             \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2885\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreply_func_executed\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2889\u001b[39m             reply=reply,\n\u001b[32m   2890\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:2162\u001b[39m, in \u001b[36mConversableAgent.generate_oai_reply\u001b[39m\u001b[34m(self, messages, sender, config, **kwargs)\u001b[39m\n\u001b[32m   2159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m processed_messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, {\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mLLM call blocked by safeguard\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m-> \u001b[39m\u001b[32m2162\u001b[39m extracted_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2164\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2165\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2166\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2167\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2169\u001b[39m \u001b[38;5;66;03m# Process LLM response\u001b[39;00m\n\u001b[32m   2170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py:2197\u001b[39m, in \u001b[36mConversableAgent._generate_oai_reply_from_client\u001b[39m\u001b[34m(self, llm_client, messages, cache, **kwargs)\u001b[39m\n\u001b[32m   2194\u001b[39m         all_messages.append(message)\n\u001b[32m   2196\u001b[39m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2197\u001b[39m response = \u001b[43mllm_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2201\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2202\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2203\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2204\u001b[39m extracted_response = llm_client.extract_text_or_completion_object(response)[\u001b[32m0\u001b[39m]\n\u001b[32m   2206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/oai/client.py:1263\u001b[39m, in \u001b[36mOpenAIWrapper.create\u001b[39m\u001b[34m(self, **config)\u001b[39m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1262\u001b[39m     request_ts = get_current_ts()\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1264\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1265\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m openai_result.is_successful:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/oai/client.py:684\u001b[39m, in \u001b[36mOpenAIClient.create\u001b[39m\u001b[34m(self, params)\u001b[39m\n\u001b[32m    682\u001b[39m     \u001b[38;5;28mself\u001b[39m._process_reasoning_model_params(params)\n\u001b[32m    683\u001b[39m params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m response = \u001b[43mcreate_or_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[38;5;66;03m# remove the system_message from the response and add it in the prompt at the start.\u001b[39;00m\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_o1:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/autogen/oai/client.py:455\u001b[39m, in \u001b[36mOpenAIClient._handle_openai_bad_request_error.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    454\u001b[39m     kwargs = OpenAIClient._patch_messages_for_deepseek_reasoner(**kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    457\u001b[39m     response_json = e.response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/G-master/lib/python3.12/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: <html>\r\n<head><title>401 Authorization Required</title></head>\r\n<body>\r\n<center><h1>401 Authorization Required</h1></center>\r\n<hr><center>openresty/1.27.4</center>\r\n<script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML=\"window.__CF$cv$params={r:'9b8bb97289e4ecc5',t:'MTc2NzU0MDA3My4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);\";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script></body>\r\n</html>"
     ]
    }
   ],
   "source": [
    "question = \"What is AI?\"\n",
    "student_response = student.run(\n",
    "    message=f\"Answer Precisely /n {question}\",\n",
    "    max_turns=1,\n",
    ")\n",
    "\n",
    "# Iterate through the chat automatically with console output\n",
    "student_response.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb72a29-73e8-4562-8143-0629d6af3166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response\n",
    "student_response.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b03ff0-8275-46bc-984e-b09446715eba",
   "metadata": {},
   "source": [
    "### Tutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e49566-dc96-437d-b2b5-8680f156315f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tutor = ConversableAgent(\n",
    "    name=\"tutor\",\n",
    "    system_message=\"You are a helpful tutor who provides clear and concise explanations suitable for a beginner.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    "    llm_config=perplexity_config  # Same config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb05174-0285-46d3-b446-ba143fae80e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e9a74b-b33f-496d-bfee-2beca414c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tutor.run(\n",
    "    message=f\"{question}\"+student_response.messages[1]['content'],\n",
    "    max_turns=1,\n",
    ")\n",
    "\n",
    "# Iterate through the chat automatically with console output\n",
    "response.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d982f5cf-c73b-4077-8c60-16299cb02bdc",
   "metadata": {},
   "source": [
    "### Start a 2-turn conversation initiated by the student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d954e232-cd34-4c9f-af74-1f10ec45d914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chat_result = student.initiate_chat(\n",
    "    recipient=tutor,                                # who the student is talking to\n",
    "    message=\"Can you explain what a neural network is?\",  # the student's question\n",
    "    max_turns=1,                                     # total number of back-and-forth messages\n",
    "    summary_method=\"reflection_with_llm\"            # generate a final summary using LLM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2417809-c13f-4325-9366-afee4c63ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Print the summary of the conversation\n",
    "print(\"\\nFinal Summary:\")\n",
    "print(chat_result.summary)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8423e9b-9b71-49eb-918b-d8a76fd5137e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Step 2: Define our two agents â€” a student and a tutor\n",
    "with perplexity_config:\n",
    "    # Create the student agent (asks questions)\n",
    "    student = ConversableAgent(\n",
    "        name=\"student\",\n",
    "        system_message=\"You are a curious student. You ask clear, specific questions to learn new concepts.\",\n",
    "        human_input_mode=\"NEVER\"  # disables manual input during chat\n",
    "    )\n",
    "\n",
    "    # Create the tutor agent (responds with beginner-friendly answers)\n",
    "    tutor = ConversableAgent(\n",
    "        name=\"tutor\",\n",
    "        system_message=\"You are a helpful tutor who provides clear and concise explanations suitable for a beginner.\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "        max_turns=3,\n",
    "    )\n",
    "\n",
    "# Step 3: Start a 2-turn conversation initiated by the student\n",
    "chat_result = student.initiate_chat(\n",
    "    recipient=tutor,                                # who the student is talking to\n",
    "    message=\"Can you explain what a neural network is?\",  # the student's question\n",
    "    max_turns=2,                                     # total number of back-and-forth messages\n",
    "    summary_method=\"reflection_with_llm\"            # generate a final summary using LLM\n",
    ")\n",
    "\n",
    "# Step 4: Print the summary of the conversation\n",
    "print(\"\\nFinal Summary:\")\n",
    "print(chat_result.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc25e26-5f4f-43d3-b115-cf5b675379b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm_config = perplexity_config\n",
    "# Create a Technical Expert Agent\n",
    "tech_expert = ConversableAgent(\n",
    "    name=\"tech_expert\",\n",
    "    system_message=\"\"\"You are a senior software engineer with expertise in Python, AI, and system design.\n",
    "    Provide technical, detailed explanations with code examples when appropriate.\n",
    "    Always consider best practices and performance implications.\"\"\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\"\n",
    ")\n",
    "\n",
    "# Create a Creative Writer Agent\n",
    "creative_writer = ConversableAgent(\n",
    "    name=\"creative_writer\",\n",
    "    system_message=\"\"\"You are a creative writer and storyteller.\n",
    "    Your responses are engaging, imaginative, and use vivid descriptions.\n",
    "    You excel at making complex topics accessible through stories and analogies.\"\"\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\"\n",
    ")\n",
    "\n",
    "# Create a Business Analyst Agent\n",
    "business_analyst = ConversableAgent(\n",
    "    name=\"business_analyst\",\n",
    "    system_message=\"\"\"You are a business analyst focused on ROI, efficiency, and strategic planning.\n",
    "    Always consider business impact, costs, and practical implementation.\n",
    "    Provide actionable recommendations with clear metrics.\"\"\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\"\n",
    ")\n",
    "\n",
    "agents = [tech_expert, creative_writer, business_analyst]\n",
    "print(\"Specialized agents created!\")\n",
    "for agent in agents:\n",
    "    print(f\"- {agent.name}: {agent.system_message.split('.')[0]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc46a7c-df0b-4013-add1-50497b0e498b",
   "metadata": {},
   "source": [
    "## Tools and Extensions<a name=\"tools\"></a>\n",
    "\n",
    "Tools extend agent capabilities beyond text conversations, allowing them to:\n",
    "- Execute code\n",
    "- Access external APIs\n",
    "- Perform calculations\n",
    "- Interact with databases\n",
    "- Generate visualizations\n",
    "\n",
    "Agents gain significant utility through tools as they provide access to external data, APIs, and functionality.  \n",
    "\n",
    "Let's explore how to integrate tools with AG2 agents:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77cb4b6-ffcd-441e-b66c-fb8e8ae868b2",
   "metadata": {},
   "source": [
    "### What is `register_function(...)` ?\n",
    "\n",
    "In AG2, `register_function(...)` is used to expose a Python function as a **tool** that can be executed by one agent on behalf of another. This enables agents to delegate tasks like computation, data processing, or external API calls.\n",
    "\n",
    "#### Purpose\n",
    "- Extend agent capabilities beyond text generation.\n",
    "- Allow agents to solve tasks through **function execution**.\n",
    "- Enable collaborative workflows between a **caller** and an **executor** agent.\n",
    "\n",
    "#### Parameters\n",
    "- **function**: A regular Python function to be used as a tool.\n",
    "- **caller**: The agent that will request the tool to be used.\n",
    "- **executor**: The agent that will actually execute the function.\n",
    "- **description** *(optional)*: A natural language description of the function for the LLM to decide when to use it.\n",
    "\n",
    "#### Example\n",
    "```python\n",
    "register_function(\n",
    "    is_prime,\n",
    "    caller=math_asker,\n",
    "    executor=math_checker,\n",
    "    description=\"Check if a number is prime. Returns Yes or No.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd93644-5346-4c9c-9e01-a190a836ca39",
   "metadata": {},
   "source": [
    "## Structured Outputs<a name=\"structured-outputs\"></a>\n",
    "\n",
    "Structured outputs ensure consistent, validated agent responses using Pydantic models.   \n",
    "\n",
    "This is crucial for:\n",
    "  \n",
    "- **Data validation**: Ensuring response format consistency\n",
    "- **API integration**: Reliable data exchange\n",
    "- **Quality assurance**: Preventing malformed outputs\n",
    "- **Type safety**: Clear data contracts\n",
    "\n",
    "**Analogy:** Like standardized hospital forms ensure doctors always record patient info the same way, structured outputs make sure agents respond in predictable, machine-readable formats.\n",
    "\n",
    "\n",
    "In AG2, structured outputs are implemented using Pydantic models and the `response_format` parameter in the `LLMConfig`.\n",
    "\n",
    "To ensure that your agent always returns outputs in a consistent structure, you define a Pydantic class (for example, `ResponseModel`) and assign it to the `response_format` argument of your configuration.\n",
    "\n",
    "This tells the underlying LLM to return a JSON-compatible response matching the defined schema.\n",
    "\n",
    "```python\n",
    "class ResponseModel(BaseModel):\n",
    "    name: str\n",
    "    status: str\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    api_type=\"openai\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    response_format=ResponseModel\n",
    ")\n",
    "```\n",
    "\n",
    "With this setup:\n",
    "\n",
    "- The agent automatically formats its responses to match the ResponseModel. \n",
    "- You donâ€™t need to prompt the LLM to format its response.\n",
    "- The response is parsed and validated by AG2.\n",
    "\n",
    "This approach is essential for reliable automation, integrations, and downstream processing.\n",
    "\n",
    "Let's implement structured outputs with AG2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355cb2a5-d7c0-4933-ad40-211c6d79a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from autogen import ConversableAgent, LLMConfig\n",
    "\n",
    "# Define the structure of the agent's output\n",
    "class TicketSummary(BaseModel):\n",
    "    customer_name: str\n",
    "    issue_type: str\n",
    "    urgency_level: str\n",
    "    recommended_action: str\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    config_list[0],\n",
    "    response_format=TicketSummary,\n",
    ")\n",
    "\n",
    "# Create the agent\n",
    "with llm_config:\n",
    "    support_agent = ConversableAgent(\n",
    "        name=\"support_agent\",\n",
    "        system_message=(\n",
    "            \"You are a support assistant. Summarize a customer ticket using:\"\n",
    "            \"\\n- customer_name\"\n",
    "            \"\\n- issue_type (e.g. login issue, billing problem, bug report)\"\n",
    "            \"\\n- urgency_level (Low, Medium, High)\"\n",
    "            \"\\n- recommended_action\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# Start a structured conversation\n",
    "support_agent.initiate_chat(\n",
    "    recipient=support_agent,\n",
    "    message=\"Ticket: John Doe is unable to reset his password and has an important meeting in 30 minutes.\",\n",
    "    max_turns=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14418971-c6d5-45eb-9fc4-da0dca79b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from perplexity import Perplexity\n",
    "\n",
    "client = Perplexity()\n",
    "\n",
    "search = client.search.create(\n",
    "    query=[\n",
    "      \"What is Comet Browser?\",\n",
    "      \"Perplexity AI\",\n",
    "      \"Perplexity Changelog\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "for result in search.results:\n",
    "    print(f\"{result.title}: {result.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0239e28-ab42-41ae-87e9-f35f71430779",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_search_tool = PerplexitySearchTool(\n",
    "    api_key=os.getenv(\"PERPLEXITY_API_KEY\"),\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "# Register the tool for LLM recommendation and execution.\n",
    "# perplexity_search_tool.register_for_llm(assistant)\n",
    "# perplexity_search_tool.register_for_execution(user_proxy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
