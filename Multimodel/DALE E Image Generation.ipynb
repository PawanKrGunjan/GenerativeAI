{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16889a44",
   "metadata": {},
   "source": [
    "## What is a Large Language Model (LLM)?\n",
    "\n",
    "Large language models (LLMs) are a type of foundation model trained on vast datasets. This extensive training enables them to understand, interpret, and generate human language, as well as other forms of content. Because of this, LLMs can perform a wide variety of tasks such as answering questions, summarizing information, translating languages, and generating creative text.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Multimodal?\n",
    "\n",
    "Multimodal refers to a model’s ability to work with multiple types of data at the same time. In artificial intelligence, multimodal systems can process and combine information from different formats, including:\n",
    "\n",
    "* **Text to image**: Creating images from written descriptions, such as with models like DALL·E\n",
    "* **Text to audio**: Transforming written text into spoken language or sounds\n",
    "* **Image to text**: Interpreting images to produce captions or descriptions\n",
    "* **Audio to text**: Converting spoken words into written text\n",
    "* **Video analysis**: Understanding video content by analyzing both visual and audio elements\n",
    "\n",
    "By integrating multiple data types, multimodal models provide a richer and more accurate understanding of content. For example, they can generate images from text prompts or describe visual scenes in words, enabling more advanced applications like interactive creative tools and context-aware conversational AI.\n",
    "\n",
    "---\n",
    "\n",
    "## What is DALL·E 2?\n",
    "\n",
    "DALL·E 2 is an artificial intelligence system created by OpenAI that generates realistic images and artwork from text prompts. Released in 2022, it builds on the original DALL·E model and offers several enhanced capabilities, including:\n",
    "\n",
    "* **Text-to-image generation** for creating visuals from natural language\n",
    "* **Image editing** to modify existing images\n",
    "* **Image variations** that produce multiple versions of the same concept\n",
    "* **Flexible resolution options**\n",
    "* **Proprietary technology**, as it is a commercial product developed by OpenAI\n",
    "\n",
    "---\n",
    "\n",
    "## What is DALL·E 3?\n",
    "\n",
    "DALL·E 3 is OpenAI’s most advanced text-to-image model, released in 2023. It significantly improves upon DALL·E 2 by offering:\n",
    "\n",
    "* **Higher-quality image generation** with greater detail and accuracy\n",
    "* **Improved prompt comprehension**, especially for complex instructions\n",
    "* **Enhanced text rendering**, allowing readable text within images\n",
    "* **Stronger artistic style replication**\n",
    "* **Advanced safety mechanisms** for responsible use\n",
    "* **Direct integration with ChatGPT**, enabling users to refine prompts interactively\n",
    "\n",
    "With higher-resolution outputs and closer alignment to user intent, DALL·E 3 is especially useful for professional design, creative projects, and detailed visual storytelling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f343f90f",
   "metadata": {},
   "source": [
    "## Image Generation\n",
    "\n",
    "The Images API provides three different endpoints, each designed for a specific image-related task:\n",
    "\n",
    "* **Generations**: Creates new images entirely from a text description.\n",
    "* **Edits**: Modifies existing images by replacing selected areas based on a text prompt.\n",
    "* **Variations**: Produces alternative versions of an existing image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e41bb08",
   "metadata": {},
   "source": [
    "## Dall-E-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cba744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from IPython import display\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.images.generate(\n",
    "    model=\"dall-e-2\",\n",
    "    prompt=\"a white siamese cat\",\n",
    "    size=\"720x720\",\n",
    "    # quality=\"standard\",\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "url = response.data[0].url\n",
    "display.Image(url=url, width=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a beautiful lake with a sunset\n",
    "\n",
    "from openai import OpenAI\n",
    "from IPython import display\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.images.generate(\n",
    "    model=\"dall-e-2\",\n",
    "    prompt=\"a beautiful lake with a sunset\",\n",
    "    size=\"1024x1024\",\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "url = response.data[0].url\n",
    "display.Image(url=url, width=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6360db7",
   "metadata": {},
   "source": [
    "## Dall-E-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af3194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from IPython import display\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.images.generate(\n",
    "    model=\"dall-e-3\",\n",
    "    prompt=\"a white siamese cat\",\n",
    "    size=\"1024x1024\",\n",
    "    quality=\"standard\",\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "url = response.data[0].url\n",
    "display.Image(url=url, width=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ffebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from openai import OpenAI\n",
    "from IPython import display\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.images.generate(\n",
    "    model=\"dall-e-3\",\n",
    "    prompt=\"a beautiful lake with a sunset\",\n",
    "    size=\"1024x1024\",\n",
    "    quality=\"standard\",\n",
    "    n=1,\n",
    ")\n",
    "\n",
    "url = response.data[0].url\n",
    "display.Image(url=url, width=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36e5be6",
   "metadata": {},
   "source": [
    "### Which Model Should You Use?\n",
    "\n",
    "DALL·E 2 and DALL·E 3 offer different capabilities for image generation.\n",
    "\n",
    "| Model    | Supported Endpoints            | Best Use Cases                                                                                                                               |\n",
    "| -------- | ------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| DALL·E 2 | Generations, Edits, Variations | Greater flexibility with image editing and variations, finer prompt control, and the ability to generate multiple images in a single request |\n",
    "| DALL·E 3 | Generations only               | Higher-quality outputs and support for larger image dimensions                                                                               |\n",
    "\n",
    "---\n",
    "\n",
    "### Image Generations\n",
    "\n",
    "The image generation endpoint allows users to create original images using text prompts. Generated images can be returned as a URL or encoded in Base64 format, with URLs expiring after a limited time.\n",
    "\n",
    "**Size and Quality Options**\n",
    "\n",
    "Square images with standard quality are the fastest to generate. By default, images are created at a resolution of `1024 × 1024` pixels, though supported sizes vary by model.\n",
    "\n",
    "| Model    | Supported Sizes (pixels)              | Quality Settings                                           | Request Limits                                                      |\n",
    "| -------- | ------------------------------------- | ---------------------------------------------------------- | ------------------------------------------------------------------- |\n",
    "| DALL·E 2 | `256×256`, `512×512`, `1024×1024`     | Standard quality only                                      | Up to 10 images per request                                         |\n",
    "| DALL·E 3 | `1024×1024`, `1024×1792`, `1792×1024` | Standard by default, with an optional high-definition mode | One image per request (additional images require parallel requests) |\n",
    "\n",
    "---\n",
    "\n",
    "### Image Edits (DALL·E 2 Only)\n",
    "\n",
    "The image editing endpoint allows users to modify or extend an image by providing the original image along with a mask that specifies which areas should be replaced. This technique is commonly known as **inpainting**.\n",
    "\n",
    "The transparent sections of the mask indicate where changes will occur. The accompanying text prompt should describe the entire updated image, not just the edited portion.\n",
    "\n",
    "To use this feature, both the original image and the mask must be square PNG files, under 4 MB in size, and share the same dimensions. Non-transparent areas of the mask are ignored during image generation and do not need to exactly match the original image.\n",
    "\n",
    "---\n",
    "\n",
    "### Image Variations (DALL·E 2 Only)\n",
    "\n",
    "The image variation endpoint generates alternative versions of a provided image. These variations retain the core structure of the original while introducing visual differences.\n",
    "\n",
    "As with image edits, the input image must be a square PNG file smaller than 4 MB.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "G-master (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
