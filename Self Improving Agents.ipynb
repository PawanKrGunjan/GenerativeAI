{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7593bb6-5729-403f-a0c2-c0a1ee0c1bac",
   "metadata": {},
   "source": [
    "# Reflection agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf22010-6e93-40e2-ab1d-4760ca4cb956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessageGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Node that generates an initial response\n",
    "def generate_answer(state):\n",
    "    # (In practice, call an LLM here)\n",
    "    answer = \"This is my first attempt.\"\n",
    "    return {\"messages\": state[\"messages\"] + [AIMessage(content=answer)]}\n",
    "\n",
    "# Node that critiques and refines the previous answer\n",
    "def critique_answer(state):\n",
    "    # (In practice, call LLM to critique)\n",
    "    critique = \"The answer is incomplete; add more detail.\"\n",
    "    return {\"messages\": state[\"messages\"] + [AIMessage(content=critique)]}\n",
    "\n",
    "builder = MessageGraph()\n",
    "builder.add_node(\"generate\", generate_answer)\n",
    "builder.add_node(\"reflect\", critique_answer)\n",
    "builder.set_entry_point(\"generate\")\n",
    "\n",
    "# Loop control: alternate until max iterations\n",
    "MAX_STEPS = 3\n",
    "def should_continue(state):\n",
    "    return \"reflect\" if len(state[\"messages\"]) < 2*MAX_STEPS else END\n",
    "\n",
    "builder.add_conditional_edges(\"generate\", should_continue)\n",
    "builder.add_edge(\"reflect\", \"generate\")\n",
    "graph = builder.compile()\n",
    "\n",
    "# Run the reflection agent\n",
    "initial_message = HumanMessage(content=\"Explain photosynthesis.\")\n",
    "result = graph.invoke({\"messages\": [initial_message]})\n",
    "print(result[\"messages\"][-1])  # Final answer or critique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de6249f-f80e-49f0-b443-724c7e621751",
   "metadata": {},
   "source": [
    "# Reflexion agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b4a18-ae72-4178-a58f-ec4c26b94d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessageGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "def draft_answer(state):\n",
    "    # (LLM draft; could also generate search query)\n",
    "    response = \"The capital of France is Paris.\"\n",
    "    return {\"messages\": state[\"messages\"] + [AIMessage(content=response)]}\n",
    "def execute_tools(state):\n",
    "    # (Simulate external info; e.g., search results)\n",
    "    info = \"París (France) - capital: Paris (en.wikipedia.org)\"\n",
    "    return {\"messages\": state[\"messages\"] + [SystemMessage(content=info)]}\n",
    "def revise_answer(state):\n",
    "    # (LLM re-evaluates answer using info)\n",
    "    revision = \"Yes, France’s capital is Paris. I've verified this.\"\n",
    "    return {\"messages\": state[\"messages\"] + [AIMessage(content=revision)]}\n",
    "builder = MessageGraph()\n",
    "builder.add_node(\"draft\", draft_answer)\n",
    "builder.add_node(\"execute_tools\", execute_tools)\n",
    "builder.add_node(\"revise\", revise_answer)\n",
    "builder.add_edge(\"draft\", \"execute_tools\")\n",
    "builder.add_edge(\"execute_tools\", \"revise\")\n",
    "# Loop control: stop after N iterations\n",
    "MAX_LOOPS = 2\n",
    "def continue_reflexion(state):\n",
    "    # Count assistant messages to determine iteration\n",
    "    iteration = sum(1 for m in state[\"messages\"] if isinstance(m, AIMessage))\n",
    "    return \"execute_tools\" if iteration <= MAX_LOOPS else END\n",
    "builder.add_conditional_edges(\"revise\", continue_reflexion)\n",
    "builder.set_entry_point(\"draft\")\n",
    "graph = builder.compile()\n",
    "initial_message = HumanMessage(content=\"What is the capital of France?\")\n",
    "result = graph.invoke({\"messages\": [initial_message]}) # Final revised answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab68f7ae-fbe5-46d5-9916-f6c4ac3c9fba",
   "metadata": {},
   "source": [
    "# ReAct agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7030b993-8aba-4131-8eb0-f85cabbdbfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Simple state with messages and a step counter\n",
    "def call_model(state):\n",
    "    # (LLM reasons; may request an action or give an answer)\n",
    "    last = state[\"messages\"][-1]\n",
    "    if \"weather\" in last:\n",
    "        # chain-of-thought leading to an action\n",
    "        thought = AIMessage(content=\"Let me find the weather for you.\")\n",
    "        return {\"messages\": state[\"messages\"] + [thought]}\n",
    "    else:\n",
    "        # final answer\n",
    "        answer = AIMessage(content=\"It's sunny in NYC today.\")\n",
    "        return {\"messages\": state[\"messages\"] + [answer]}\n",
    "\n",
    "def call_tool(state):\n",
    "    # (Simulate a weather API/tool result)\n",
    "    tool_result = AIMessage(content=\"Weather(temperature=75F, condition=sunny)\")\n",
    "    return {\"messages\": state[\"messages\"] + [tool_result]}\n",
    "\n",
    "# Decide whether to act or finish based on last message\n",
    "def next_step(state):\n",
    "    last = state[\"messages\"][-1]\n",
    "    if \"find the weather\" in last:\n",
    "        return \"tools\"\n",
    "    return \"end\"\n",
    "\n",
    "graph = StateGraph(dict)  # using a plain dict state\n",
    "graph.add_node(\"think\", call_model)\n",
    "graph.add_node(\"act\", call_tool)\n",
    "graph.set_entry_point(\"think\")\n",
    "# If the model’s message triggers an action, go to 'act'; else end.\n",
    "graph.add_conditional_edges(\"think\", next_step, {\"tools\": \"act\", \"end\": END})\n",
    "graph.add_edge(\"act\", \"think\")\n",
    "compiled = graph.compile()\n",
    "\n",
    "result = compiled.invoke({\"messages\": [HumanMessage(content=\"What is the weather in NYC?\")]})\n",
    "print(result[\"messages\"][-1])  # Final assistant answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da5cb88-89ff-4422-bdae-d84f944b32ac",
   "metadata": {},
   "source": [
    "## Comparison of Agent Styles\n",
    "\n",
    "| Aspect | Reflection Agent | Reflexion Agent | ReAct Agent |\n",
    "|--------|------------------|-----------------|-------------|\n",
    "| **Core idea** | Model critiques its own answer | Model critiques with external feedback and citations | Model reasons and acts (calls tools) in loop |\n",
    "| **Structure** | Generator → Reflector → (loop) | Draft → (Search/Tool) → Revisor → (loop) | LLM → (conditional Tool call) → LLM → … |\n",
    "| **Graph components** | 2 nodes (generate, reflect) | 3+ nodes (draft, execute tools, revise) | 2 nodes (think, act) with conditional branching |\n",
    "| **Feedback source** | Internal (LLM self-review) | External (tool or search results + LLM review) | External (tool calls informed by model reasoning) |\n",
    "| **Benefits** | Simple setup; improves coherence & detail | High accuracy; enforces evidence and completeness | Flexible tool use; handles complex tasks |\n",
    "| **Drawbacks** | May plateau (no new info); extra compute | More complex and slow (searches/tools each loop) | Requires designing tools; complexity in prompts |\n",
    "| **Use cases** | Refining essays, content drafts | Fact-checking, coding, QA with citations | Question answering with APIs, step-by-step tasks |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bdc180-55b8-4483-8ac4-e3cd6844e265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
