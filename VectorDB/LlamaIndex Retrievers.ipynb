{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49499f33-fc91-4711-9251-d59c9df76e6e",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <h1>Advanced Retrievers in LlamaIndex</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e085d48a-6c72-4068-8a4f-3eaf18435a0b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84ac23bf-8767-4962-9827-1cb2ee027b12",
   "metadata": {},
   "source": [
    "pip install llama-index\n",
    "pip install llama-index-embeddings-huggingface\n",
    "pip install llama-index-retrievers-bm25\n",
    "pip install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0596bc63-ff0e-492d-8682-05f6e5d6162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 11:45:13.969237: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Optional\n",
    "import asyncio\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core LlamaIndex imports\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex, \n",
    "    SimpleDirectoryReader, \n",
    "    Document,\n",
    "    Settings,\n",
    "    DocumentSummaryIndex,\n",
    "    KeywordTableIndex\n",
    ")\n",
    "from llama_index.core.retrievers import (\n",
    "    BaseRetriever,\n",
    "    VectorIndexRetriever,\n",
    "    AutoMergingRetriever,\n",
    "    RecursiveRetriever,\n",
    "    QueryFusionRetriever\n",
    ")\n",
    "from llama_index.core.indices.document_summary import (\n",
    "    DocumentSummaryIndexLLMRetriever,\n",
    "    DocumentSummaryIndexEmbeddingRetriever,\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter, HierarchicalNodeParser\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.embeddings import BaseEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Advanced retriever imports\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Sentence transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Statistical libraries for fusion techniques\n",
    "try:\n",
    "    from scipy import stats\n",
    "    SCIPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SCIPY_AVAILABLE = False\n",
    "    print(\"âš ï¸ scipy not available - some advanced fusion features will be limited\")\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bdb685e-063f-4004-afae-3c7caa1e084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_llm(max_tokens=500):\n",
    "    ollama_llm = Ollama(\n",
    "        model='llama3.2:latest',\n",
    "        request_timeout=120.0,\n",
    "        temperature=0.0,\n",
    "        max_output_tokens=max_tokens,\n",
    "        context_window=2000,           # newer llama3.2 supports more\n",
    "        format=\"json\",               # â† try this if your ollama version supports it\n",
    "    )\n",
    "    return ollama_llm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03589c27-7614-4737-838d-923b46f264a0",
   "metadata": {},
   "source": [
    "llm = ollama_llm()\n",
    "resp = llm.complete(\"Who is Pawan Kumar Gunjan?\")\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed7de6c-862e-401b-a454-2780c0686ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Initializing HuggingFace embeddings...\n",
      "âœ… HuggingFace embeddings initialized!\n",
      "ðŸ”§ Initializing watsonx.ai LLM...\n",
      "âœ… watsonx.ai LLM and embeddings configured!\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model first\n",
    "print(\"ðŸ”§ Initializing HuggingFace embeddings...\")\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\", device = 'cpu'\n",
    ")\n",
    "print(\"âœ… HuggingFace embeddings initialized!\")\n",
    "\n",
    "# Setup with watsonx.ai\n",
    "print(\"ðŸ”§ Initializing watsonx.ai LLM...\")\n",
    "llm = ollama_llm()\n",
    "\n",
    "# Configure global settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "print(\"âœ… watsonx.ai LLM and embeddings configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b6fbb7c-7bbc-417e-b254-da8124f803b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Loaded 10 sample documents\n",
      "ðŸ” Prepared 7 consistent demo queries\n",
      "1. Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\n",
      "2. Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\n",
      "3. Natural language processing enables computers to understand, interpret, and generate human language.\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Sample data for the lab - AI/ML focused documents\n",
    "SAMPLE_DOCUMENTS = [\n",
    "    \"Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\",\n",
    "    \"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\",\n",
    "    \"Natural language processing enables computers to understand, interpret, and generate human language.\",\n",
    "    \"Computer vision allows machines to interpret and understand visual information from the world.\",\n",
    "    \"Reinforcement learning is a type of machine learning where agents learn to make decisions through rewards and penalties.\",\n",
    "    \"Supervised learning uses labeled training data to learn a mapping from inputs to outputs.\",\n",
    "    \"Unsupervised learning finds hidden patterns in data without labeled examples.\",\n",
    "    \"Transfer learning leverages knowledge from pre-trained models to improve performance on new tasks.\",\n",
    "    \"Generative AI can create new content including text, images, code, and more.\",\n",
    "    \"Large language models are trained on vast amounts of text data to understand and generate human-like text.\"\n",
    "]\n",
    "\n",
    "# Consistent query examples used throughout the lab\n",
    "DEMO_QUERIES = {\n",
    "    \"basic\": \"What is machine learning?\",\n",
    "    \"technical\": \"neural networks deep learning\", \n",
    "    \"learning_types\": \"different types of learning\",\n",
    "    \"advanced\": \"How do neural networks work in deep learning?\",\n",
    "    \"applications\": \"What are the applications of AI?\",\n",
    "    \"comprehensive\": \"What are the main approaches to machine learning?\",\n",
    "    \"specific\": \"supervised learning techniques\"\n",
    "}\n",
    "\n",
    "print(f\"ðŸ“„ Loaded {len(SAMPLE_DOCUMENTS)} sample documents\")\n",
    "print(f\"ðŸ” Prepared {len(DEMO_QUERIES)} consistent demo queries\")\n",
    "for i, doc in enumerate(SAMPLE_DOCUMENTS[:3], 1):\n",
    "    print(f\"{i}. {doc}\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8739b1f-5b0c-497e-b3e8-1cc632595bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Initializing Advanced Retrievers Lab...\n",
      "ðŸ“Š Creating indexes...\n",
      "current doc id: b34ab465-4a91-40fd-a8e7-a40f59cfee94\n",
      "current doc id: 364b2795-3ac1-4abb-a8be-c46310debed5\n",
      "current doc id: 7d2ed37b-1f5b-4134-9cbf-af0f7f245dae\n",
      "current doc id: 4542484b-7b5c-4e2b-9bc0-c0d22f7964f5\n",
      "current doc id: 7df0a828-b3f7-4475-be57-87fe132cff3e\n",
      "current doc id: cc44e17a-e4f0-48e0-be93-2ca554df19e9\n",
      "current doc id: 750f7098-9eee-4f31-a9f9-aeffb2a837b0\n",
      "current doc id: 9f32891d-6cd6-4732-b9de-49140148af5d\n",
      "current doc id: 5e41b1ed-7746-4485-99c7-2fbc5c806413\n",
      "current doc id: 049e276e-9b9f-47c1-be9a-d57df1951bcb\n",
      "âœ… Advanced Retrievers Lab Initialized!\n",
      "ðŸ“„ Loaded 10 documents\n",
      "ðŸ”¢ Created 10 nodes\n"
     ]
    }
   ],
   "source": [
    "class AdvancedRetrieversLab:\n",
    "    def __init__(self):\n",
    "        print(\"ðŸš€ Initializing Advanced Retrievers Lab...\")\n",
    "        self.documents = [Document(text=text) for text in SAMPLE_DOCUMENTS]\n",
    "        self.nodes = SentenceSplitter().get_nodes_from_documents(self.documents)\n",
    "        \n",
    "        print(\"ðŸ“Š Creating indexes...\")\n",
    "        # Create various indexes\n",
    "        self.vector_index = VectorStoreIndex.from_documents(self.documents)\n",
    "        self.document_summary_index = DocumentSummaryIndex.from_documents(self.documents)\n",
    "        self.keyword_index = KeywordTableIndex.from_documents(self.documents)\n",
    "        \n",
    "        print(\"âœ… Advanced Retrievers Lab Initialized!\")\n",
    "        print(f\"ðŸ“„ Loaded {len(self.documents)} documents\")\n",
    "        print(f\"ðŸ”¢ Created {len(self.nodes)} nodes\")\n",
    "\n",
    "# Initialize the lab\n",
    "lab = AdvancedRetrieversLab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d94fe5-d2dc-4a52-ae21-c6a3e94db563",
   "metadata": {},
   "source": [
    "# Core Retriever Demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910997be-3aba-44e3-a416-e62b7a67219c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Vector Index Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d7e3c0-9d9c-4aaf-83a4-dd8f431c5147",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "The Vector-Based Retriever relies on vector embeddings to identify content that is semantically similar, making it the most widely used approach for general semantic search and the foundation of most modern retrieval-augmented generation (RAG) systems.\n",
    "\n",
    "**How it operates:**\n",
    "- Source documents are divided into smaller chunks (nodes)\n",
    "- Each chunk is transformed into a dense vector using a selected embedding model\n",
    "- The user's query is also converted into a vector\n",
    "- The system returns the most relevant chunks, ranked according to cosine similarity between query and document vectors\n",
    "- Embeddings are typically generated in reasonably large batches (often around 2000 items)\n",
    "\n",
    "**Best suited for:**\n",
    "- General semantic search tasks (the default choice in most applications)\n",
    "- Discovering content that shares similar meaning or ideas\n",
    "- RAG systems that depend on contextual understanding\n",
    "- Situations where natural language understanding matters more than precise keyword matching\n",
    "\n",
    "**Main characteristics:**\n",
    "- Maintains vector representations for every document segment\n",
    "- Excels at retrieving information based on conceptual similarity rather than literal text matches\n",
    "- Serves as the standard retrieval mechanism in many LLM-powered applications\n",
    "\n",
    "**Advantages:**\n",
    "- Strong ability to capture meaning, nuance, and context\n",
    "- Effectively handles synonyms, paraphrases, and conceptually related content\n",
    "- Performs well with conversational or free-form natural language questions\n",
    "\n",
    "**Limitations:**\n",
    "- May overlook results that contain very specific keywords when exact phrasing is important\n",
    "- Performance heavily depends on the quality of the embedding model being used\n",
    "- Can become resource-intensive (memory and compute) when dealing with very large document sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78a4e0a7-c784-4a85-8c37-52921e130ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. VECTOR INDEX RETRIEVER\n",
      "============================================================\n",
      "Query: What is machine learning?\n",
      "Retrieved 3 nodes:\n",
      "1. Score: 0.8700\n",
      "   Text: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn fr...\n",
      "\n",
      "2. Score: 0.7644\n",
      "   Text: Reinforcement learning is a type of machine learning where agents learn to make decisions through re...\n",
      "\n",
      "3. Score: 0.6979\n",
      "   Text: Supervised learning uses labeled training data to learn a mapping from inputs to outputs....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"1. VECTOR INDEX RETRIEVER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic vector retriever\n",
    "vector_retriever = VectorIndexRetriever(\n",
    "    index=lab.vector_index,\n",
    "    similarity_top_k=3\n",
    ")\n",
    "\n",
    "# Alternative creation method\n",
    "alt_retriever = lab.vector_index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "query = DEMO_QUERIES[\"basic\"]  # \"What is machine learning?\"\n",
    "nodes = vector_retriever.retrieve(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Retrieved {len(nodes)} nodes:\")\n",
    "for i, node in enumerate(nodes, 1):\n",
    "    print(f\"{i}. Score: {node.score:.4f}\")\n",
    "    print(f\"   Text: {node.text[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a08700d-d4e8-4f0a-b50a-2e02bca215c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. BM25 Retriever â€“ Precise Keyword Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d245bc-21a0-4b81-8875-04ec193e77d1",
   "metadata": {},
   "source": [
    "\n",
    "BM25 is a powerful, statistically grounded keyword retrieval algorithm that significantly improves upon classic TF-IDF. It remains one of the most widely deployed ranking methods in modern search engines, including systems like Elasticsearch, Solr, and many enterprise search platforms.\n",
    "\n",
    "### Quick Review: What TF-IDF Does (and Where It Falls Short)\n",
    "\n",
    "TF-IDF (Term Frequency Ã— Inverse Document Frequency) was a major advancement in information retrieval:\n",
    "\n",
    "- **Term Frequency (TF)** â€” counts how frequently a term appears in a single document (usually normalized by document length)  \n",
    "  Example: \"algorithm\" appearing 5 times in a 200-word document â†’ TF = 5/200 = 0.025\n",
    "\n",
    "- **Inverse Document Frequency (IDF)** â€” measures how rare or distinctive a term is across the entire collection  \n",
    "  Example: If \"algorithm\" appears in 5 documents out of 10,000 â†’ IDF = log(10,000/5) â‰ˆ 7.6  \n",
    "  Very common words (like \"and\", \"is\") receive very low IDF values\n",
    "\n",
    "- **TF-IDF score** = TF Ã— IDF  \n",
    "  â†’ Gives high importance to terms that are frequent in a document but rare overall\n",
    "\n",
    "While effective, TF-IDF has two major practical weaknesses:\n",
    "- Repeated occurrences of a term keep increasing the score without limit (no saturation)\n",
    "- Longer documents tend to receive unfairly high scores simply because they contain more words\n",
    "\n",
    "### How BM25 Solves These Problems\n",
    "\n",
    "BM25 (Best Matching 25) introduces smart refinements that make it much more effective in real-world search:\n",
    "\n",
    "1. **Term Frequency Saturation**  \n",
    "   - Instead of letting term frequency grow linearly forever, BM25 applies a non-linear saturation curve  \n",
    "   - Going from 1 to 10 occurrences gives a big score boost  \n",
    "   - Going from 10 to 100 gives much less additional benefit  \n",
    "   â†’ Prevents very long, repetitive documents from dominating results\n",
    "\n",
    "2. **Document Length Normalization**  \n",
    "   - Adjusts scores based on how much longer or shorter a document is compared to the collection average  \n",
    "   - Short documents arenâ€™t unfairly penalized  \n",
    "   - Very long documents donâ€™t get an automatic advantage  \n",
    "   - Controlled by parameter **b** (typically ~0.75)\n",
    "\n",
    "3. **Tunable Hyperparameters**  \n",
    "   - **k1** (~1.2â€“2.0) â€” controls how quickly term frequency saturation kicks in  \n",
    "   - **b** (~0.75) â€” controls strength of length normalization (0 = none, 1 = full normalization)\n",
    "\n",
    "### Best Use Cases for BM25\n",
    "\n",
    "Choose BM25 when you need:\n",
    "\n",
    "- High precision for exact keyword and phrase matching\n",
    "- Searching technical manuals, API documentation, or code repositories\n",
    "- Legal, medical, or patent documents where specific terminology is critical\n",
    "- E-commerce product search with model numbers, SKUs, or precise attributes\n",
    "- Any domain where users expect literal term matching over conceptual similarity\n",
    "\n",
    "### Key Strengths\n",
    "\n",
    "- Extremely fast ranking (no neural networks or heavy computation)\n",
    "- Outstanding performance on exact-match and keyword-heavy queries\n",
    "- No training data or model fine-tuning required\n",
    "- Very interpretable â€” you can explain why a document ranked highly\n",
    "- Battle-tested in production for decades\n",
    "\n",
    "### Main Limitations\n",
    "\n",
    "- Completely lacks semantic understanding (cannot match \"car\" â†” \"automobile\" or \"run\" â†” \"jogging\")\n",
    "- Sensitive to spelling mistakes, word variations, and morphological differences\n",
    "- No awareness of sentence structure, word order, or broader context\n",
    "- Requires thoughtful tuning of k1 and b parameters for best results in each domain\n",
    "\n",
    "In summary, BM25 remains the go-to choice whenever precise, fast, and reliable keyword-based retrieval is more important than semantic or conceptual matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dce94ad2-d564-47be-81eb-5749c78f74f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "2. BM25 RETRIEVER\n",
      "============================================================\n",
      "Query: neural networks deep learning\n",
      "BM25 analyzes exact keyword matches with sophisticated scoring\n",
      "Retrieved 3 nodes:\n",
      "1. BM25 Score: 2.5203\n",
      "   Text: Deep learning uses neural networks with multiple layers to model and understand complex patterns in ...\n",
      "   â†’ Found terms: ['neural', 'networks', 'deep', 'learning']\n",
      "\n",
      "2. BM25 Score: 0.3372\n",
      "   Text: Reinforcement learning is a type of machine learning where agents learn to make decisions through re...\n",
      "   â†’ Found terms: ['learning']\n",
      "\n",
      "3. BM25 Score: 0.3024\n",
      "   Text: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn fr...\n",
      "   â†’ Found terms: ['learning']\n",
      "\n",
      "BM25 vs TF-IDF Comparison:\n",
      "TF-IDF Problem: Linear term frequency scaling\n",
      "  Example: 10 occurrences â†’ score of 10, 100 occurrences â†’ score of 100\n",
      "BM25 Solution: Saturation function\n",
      "  Example: 10 occurrences â†’ high score, 100 occurrences â†’ slightly higher score\n",
      "\n",
      "TF-IDF Problem: No document length consideration\n",
      "  Example: Long documents dominate results\n",
      "BM25 Solution: Length normalization (b parameter)\n",
      "  Example: Scores adjusted based on document length vs. average\n",
      "\n",
      "Key BM25 Parameters:\n",
      "- k1 â‰ˆ 1.2: Term frequency saturation (how quickly scores plateau)\n",
      "- b â‰ˆ 0.75: Document length normalization (0=none, 1=full)\n",
      "- IDF weighting: Rare terms get higher scores\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"2. BM25 RETRIEVER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    import Stemmer\n",
    "    \n",
    "    # Create BM25 retriever with default parameters\n",
    "    bm25_retriever = BM25Retriever.from_defaults(\n",
    "        nodes=lab.nodes,\n",
    "        similarity_top_k=3,\n",
    "        stemmer=Stemmer.Stemmer(\"english\"),\n",
    "        language=\"english\"\n",
    "    )\n",
    "    \n",
    "    query = DEMO_QUERIES[\"technical\"]  # \"neural networks deep learning\"\n",
    "    nodes = bm25_retriever.retrieve(query)\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"BM25 analyzes exact keyword matches with sophisticated scoring\")\n",
    "    print(f\"Retrieved {len(nodes)} nodes:\")\n",
    "    \n",
    "    for i, node in enumerate(nodes, 1):\n",
    "        score = node.score if hasattr(node, 'score') and node.score else 0\n",
    "        print(f\"{i}. BM25 Score: {score:.4f}\")\n",
    "        print(f\"   Text: {node.text[:100]}...\")\n",
    "        \n",
    "        # Highlight which query terms appear in the text\n",
    "        text_lower = node.text.lower()\n",
    "        query_terms = query.lower().split()\n",
    "        found_terms = [term for term in query_terms if term in text_lower]\n",
    "        if found_terms:\n",
    "            print(f\"   â†’ Found terms: {found_terms}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"BM25 vs TF-IDF Comparison:\")\n",
    "    print(\"TF-IDF Problem: Linear term frequency scaling\")\n",
    "    print(\"  Example: 10 occurrences â†’ score of 10, 100 occurrences â†’ score of 100\")\n",
    "    print(\"BM25 Solution: Saturation function\")\n",
    "    print(\"  Example: 10 occurrences â†’ high score, 100 occurrences â†’ slightly higher score\")\n",
    "    print()\n",
    "    print(\"TF-IDF Problem: No document length consideration\")\n",
    "    print(\"  Example: Long documents dominate results\")\n",
    "    print(\"BM25 Solution: Length normalization (b parameter)\")\n",
    "    print(\"  Example: Scores adjusted based on document length vs. average\")\n",
    "    print()\n",
    "    print(\"Key BM25 Parameters:\")\n",
    "    print(\"- k1 â‰ˆ 1.2: Term frequency saturation (how quickly scores plateau)\")\n",
    "    print(\"- b â‰ˆ 0.75: Document length normalization (0=none, 1=full)\")\n",
    "    print(\"- IDF weighting: Rare terms get higher scores\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"âš ï¸ BM25Retriever requires 'pip install PyStemmer'\")\n",
    "    print(\"Demonstrating BM25 concepts with fallback vector search...\")\n",
    "    \n",
    "    fallback_retriever = lab.vector_index.as_retriever(similarity_top_k=3)\n",
    "    query = DEMO_QUERIES[\"technical\"]\n",
    "    nodes = fallback_retriever.retrieve(query)\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"(Using vector fallback to demonstrate BM25 concepts)\")\n",
    "    \n",
    "    for i, node in enumerate(nodes, 1):\n",
    "        print(f\"{i}. Vector Score: {node.score:.4f}\")\n",
    "        print(f\"   Text: {node.text[:100]}...\")\n",
    "        \n",
    "        # Demonstrate TF-IDF concept manually\n",
    "        text_lower = node.text.lower()\n",
    "        query_terms = query.lower().split()\n",
    "        found_terms = [term for term in query_terms if term in text_lower]\n",
    "        \n",
    "        if found_terms:\n",
    "            print(f\"   â†’ BM25 would boost this result for terms: {found_terms}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"BM25 Concept Demonstration:\")\n",
    "    print(\"1. TF-IDF Foundation:\")\n",
    "    print(\"   - Term Frequency: How often words appear in document\")\n",
    "    print(\"   - Inverse Document Frequency: How rare words are across collection\")\n",
    "    print(\"   - TF-IDF = TF Ã— IDF (balances frequency vs rarity)\")\n",
    "    print()\n",
    "    print(\"2. BM25 Improvements:\")\n",
    "    print(\"   - Saturation: Prevents over-scoring repeated terms\")\n",
    "    print(\"   - Length normalization: Prevents long document bias\")\n",
    "    print(\"   - Tunable parameters: k1 (saturation) and b (length adjustment)\")\n",
    "    print()\n",
    "    print(\"3. Real-world Usage:\")\n",
    "    print(\"   - Elasticsearch default scoring function\")\n",
    "    print(\"   - Apache Lucene/Solr standard\")\n",
    "    print(\"   - Used in 83% of text-based recommender systems\")\n",
    "    print(\"   - Developed by Robertson & SpÃ¤rck Jones at City University London\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709d19b-14b7-4b95-9542-6f3ec5fe0cac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Document Summaryâ€“Based Retriever â€“ Smart Document-Level Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825a53c-b86f-4168-b98e-4cef5feb37ab",
   "metadata": {},
   "source": [
    "\n",
    "The Document Summaryâ€“Based Retriever is a clever two-stage approach designed for working efficiently with very large document collections.  \n",
    "Instead of comparing the query directly against every full document, it first uses concise summaries to quickly identify the most promising documents â€” then returns the **complete original documents** (not the summaries).\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **During indexing**:\n",
    "   - Each full document is processed to create a short, meaningful summary (usually via an LLM)\n",
    "   - Both the summary and the original document are stored\n",
    "   - The summary is also converted into a vector embedding (for embedding-based retrieval)\n",
    "\n",
    "2. **During retrieval**:\n",
    "   - The system compares the user query against the **summaries only** (much faster than scanning full documents)\n",
    "   - It selects the most relevant documents based on the summaries\n",
    "   - Finally, it returns the **full, original versions** of those selected documents\n",
    "\n",
    "This two-step filtering dramatically reduces the number of documents that need detailed processing, making it ideal for massive or diverse collections.\n",
    "\n",
    "### Two Main Retrieval Modes\n",
    "\n",
    "1. **LLM-Powered Summary Retriever** (`DocumentSummaryIndexLLMRetriever`)\n",
    "   - An LLM evaluates the query against each document summary\n",
    "   - Makes intelligent, context-aware decisions about relevance\n",
    "   - Better suited for complex, nuanced, or multi-faceted questions\n",
    "   - More accurate but slower and more expensive\n",
    "\n",
    "2. **Embedding-Based Summary Retriever** (`DocumentSummaryIndexEmbeddingRetriever`)\n",
    "   - Uses vector similarity between the query embedding and summary embeddings\n",
    "   - Much faster and more cost-efficient\n",
    "   - Excellent when semantic similarity between query and high-level document topic is sufficient\n",
    "\n",
    "### Best Use Cases\n",
    "\n",
    "This retriever shines when you are dealing with:\n",
    "\n",
    "- Very large document sets (thousands to millions of documents)\n",
    "- Collections containing many distinct topics or subject areas\n",
    "- Multi-document question answering over heterogeneous sources\n",
    "- Situations where loading every document into the LLM context window is impossible\n",
    "- Enterprise knowledge bases, research paper archives, legal document repositories, technical manuals across many products, etc.\n",
    "\n",
    "### Important Configuration Settings\n",
    "\n",
    "- `choice_top_k` (for LLM mode) â€” how many documents the LLM should select\n",
    "- `similarity_top_k` (for embedding mode) â€” how many most similar summaries to keep\n",
    "- Default value is usually 1 â†’ increase to 3â€“10 when you want multiple candidate documents\n",
    "\n",
    "**Critical reminder**: The system **always returns the full original documents** â€” summaries are used only for fast filtering/selection.\n",
    "\n",
    "### Key Advantages\n",
    "\n",
    "- Greatly reduces the number of documents processed in detail\n",
    "- Excellent at handling diverse, topic-scattered collections\n",
    "- Preserves complete context and all original details in final results\n",
    "- Very effective for scaling retrieval to huge corpora\n",
    "\n",
    "### Main Limitations\n",
    "\n",
    "- Requires generating summaries during indexing (usually needs LLM â†’ adds time and cost at setup)\n",
    "- Quality of retrieval depends heavily on how good the summaries are\n",
    "- Some fine details or specific passages may be overlooked if not captured well in the summary\n",
    "- LLM-based variant is slower and more expensive than pure vector or keyword methods\n",
    "\n",
    "In short: use Document Summaryâ€“Based retrieval when you need to intelligently narrow down a huge, varied document collection before doing detailed matching â€” without ever losing access to the complete original content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceeb4f9f-af5b-49c3-b510-e5dca93b380d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "3. DOCUMENT SUMMARY INDEX RETRIEVERS\n",
      "============================================================\n",
      "Query: different types of learning\n",
      "\n",
      "A) LLM-based Document Summary Retriever:\n",
      "Uses LLM to select relevant documents based on summaries\n",
      "Retrieved 0 nodes\n",
      "B) Embedding-based Document Summary Retriever:\n",
      "Uses vector similarity between query and document summaries\n",
      "Retrieved 3 nodes\n",
      "1. (Document summary)\n",
      "   Text: Reinforcement learning is a type of machine learning where agents learn to make ...\n",
      "\n",
      "2. (Document summary)\n",
      "   Text: Unsupervised learning finds hidden patterns in data without labeled examples....\n",
      "\n",
      "Document Summary Index workflow:\n",
      "1. Generates summaries for each document using LLM\n",
      "2. Uses summaries to select relevant documents\n",
      "3. Returns full content from selected documents\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"3. DOCUMENT SUMMARY INDEX RETRIEVERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# LLM-based document summary retriever\n",
    "doc_summary_retriever_llm = DocumentSummaryIndexLLMRetriever(\n",
    "    lab.document_summary_index,\n",
    "    choice_top_k=3  # Number of documents to select\n",
    ")\n",
    "\n",
    "# Embedding-based document summary retriever  \n",
    "doc_summary_retriever_embedding = DocumentSummaryIndexEmbeddingRetriever(\n",
    "    lab.document_summary_index,\n",
    "    similarity_top_k=3  # Number of documents to select\n",
    ")\n",
    "\n",
    "query = DEMO_QUERIES[\"learning_types\"]  # \"different types of learning\"\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "print(\"\\nA) LLM-based Document Summary Retriever:\")\n",
    "print(\"Uses LLM to select relevant documents based on summaries\")\n",
    "try:\n",
    "    nodes_llm = doc_summary_retriever_llm.retrieve(query)\n",
    "    print(f\"Retrieved {len(nodes_llm)} nodes\")\n",
    "    for i, node in enumerate(nodes_llm[:2], 1):\n",
    "        print(f\"{i}. Score: {node.score:.4f}\" if hasattr(node, 'score') and node.score else f\"{i}. (Document summary)\")\n",
    "        print(f\"   Text: {node.text[:80]}...\")\n",
    "        print()\n",
    "except Exception as e:\n",
    "    print(f\"LLM-based retrieval demo: {str(e)[:100]}...\")\n",
    "\n",
    "print(\"B) Embedding-based Document Summary Retriever:\")\n",
    "print(\"Uses vector similarity between query and document summaries\")\n",
    "try:\n",
    "    nodes_emb = doc_summary_retriever_embedding.retrieve(query)\n",
    "    print(f\"Retrieved {len(nodes_emb)} nodes\")\n",
    "    for i, node in enumerate(nodes_emb[:2], 1):\n",
    "        print(f\"{i}. Score: {node.score:.4f}\" if hasattr(node, 'score') and node.score else f\"{i}. (Document summary)\")\n",
    "        print(f\"   Text: {node.text[:80]}...\")\n",
    "        print()\n",
    "except Exception as e:\n",
    "    print(f\"Embedding-based retrieval demo: {str(e)[:100]}...\")\n",
    "\n",
    "print(\"Document Summary Index workflow:\")\n",
    "print(\"1. Generates summaries for each document using LLM\")\n",
    "print(\"2. Uses summaries to select relevant documents\")\n",
    "print(\"3. Returns full content from selected documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6d4d1a-afe2-445a-b7c7-ebfac97d802f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Auto-Merging Retriever â€“ Smart Context Recovery for Long Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6a02a-752d-4ac0-8f18-51e90f8259fc",
   "metadata": {},
   "source": [
    "\n",
    "The Auto-Merging Retriever is a sophisticated retrieval strategy that solves one of the biggest problems in vector search: **loss of surrounding context** when documents are split into small chunks.\n",
    "\n",
    "It automatically decides whether to return small, precise pieces or larger, more contextual sections â€” depending on how many related small pieces were found.\n",
    "\n",
    "### Core Mechanism\n",
    "\n",
    "1. **Hierarchical Chunking**  \n",
    "   Documents are split into multiple levels of granularity:  \n",
    "   - Large parent chunks (e.g., whole sections or pages)  \n",
    "   - Smaller child chunks (e.g., paragraphs or sentences)\n",
    "\n",
    "2. **Dual Storage Strategy**  \n",
    "   - Only the **small child chunks** are embedded and indexed in the vector store â†’ enables very precise semantic matching  \n",
    "   - The **larger parent chunks** are kept in a separate document store (not embedded)\n",
    "\n",
    "3. **Intelligent Merging Logic**  \n",
    "   - When a query retrieves several child chunks  \n",
    "   - If enough of those children belong to the same parent (threshold is configurable)  \n",
    "   - â†’ The system discards the individual children and returns the **complete parent chunk** instead\n",
    "\n",
    "This behavior automatically gives you:  \n",
    "â†’ Precise matching when only one small piece is relevant  \n",
    "â†’ Richer context when multiple related pieces are triggered\n",
    "\n",
    "### Best Use Cases\n",
    "\n",
    "Choose the Auto-Merging Retriever when working with:\n",
    "\n",
    "- Long-form content: research papers, technical manuals, books, annual reports  \n",
    "- Legal contracts, patents, regulations â€” where context is critical  \n",
    "- Documents that naturally have structure (chapters â†’ sections â†’ paragraphs)  \n",
    "- Situations where small fixed-size chunks frequently cut important ideas in half  \n",
    "- Applications that need both pinpoint accuracy and coherent, readable context\n",
    "\n",
    "### Typical Configuration Options\n",
    "\n",
    "- `chunk_sizes` â€” list of sizes from largest to smallest (example: `[1024, 512, 256]`)  \n",
    "- `chunk_overlap` â€” number of tokens to overlap between chunks (helps continuity)  \n",
    "- Merge threshold â€” minimum number of child chunks needed from the same parent to trigger merge (default often 2â€“3)  \n",
    "- Vector store holds child nodes, document store holds parents\n",
    "\n",
    "### Major Advantages\n",
    "\n",
    "- Automatically recovers lost context without manual post-processing  \n",
    "- Greatly reduces \"fragmented answer\" problem common in basic chunking  \n",
    "- Keeps very fine-grained search precision when needed  \n",
    "- Adapts dynamically to query complexity and match distribution  \n",
    "- Works especially well on documents with logical hierarchy\n",
    "\n",
    "### Main Drawbacks\n",
    "\n",
    "- More complex implementation than simple vector or keyword retrievers  \n",
    "- Requires thoughtful choice of multiple chunk sizes  \n",
    "- Increased storage needs (multiple representations of the same content)  \n",
    "- Less benefit on very short or completely flat documents  \n",
    "- Initial indexing takes longer due to multiple chunking passes\n",
    "\n",
    "**In essence**:  \n",
    "The Auto-Merging Retriever gives you the best of both worlds â€” razor-sharp relevance from small chunks combined with the full surrounding context from larger sections â€” automatically deciding which level is most appropriate for each query.\n",
    "\n",
    "https://docs.llamaindex.ai/en/stable/examples/retrievers/auto_merging_retriever/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60854956-eccf-4e98-9352-d7329fb40529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "4. AUTO MERGING RETRIEVER\n",
      "============================================================\n",
      "> Merging 1 nodes into parent node.\n",
      "> Parent node id: a10042cb-0551-4144-86eb-08e451099963.\n",
      "> Parent node text: Supervised learning uses labeled training data to learn a mapping from inputs to outputs.\n",
      "\n",
      "> Merging 1 nodes into parent node.\n",
      "> Parent node id: a3774f1c-8712-4ad7-8bf6-70f54c7d8478.\n",
      "> Parent node text: Supervised learning uses labeled training data to learn a mapping from inputs to outputs.\n",
      "\n",
      "Query: How do neural networks work in deep learning?\n",
      "Auto-merged to 2 nodes\n",
      "1. Score: 0.8570\n",
      "   Text: Deep learning uses neural networks with multiple layers to model and understand complex patterns in data....\n",
      "\n",
      "2. Score: 0.6956\n",
      "   Text: Supervised learning uses labeled training data to learn a mapping from inputs to outputs....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"4. AUTO MERGING RETRIEVER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create hierarchical nodes\n",
    "node_parser = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes=[512, 256, 128]\n",
    ")\n",
    "\n",
    "hier_nodes = node_parser.get_nodes_from_documents(lab.documents)\n",
    "\n",
    "# Create storage context with all nodes\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.vector_stores import SimpleVectorStore\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(hier_nodes)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "\n",
    "# Create base index\n",
    "base_index = VectorStoreIndex(hier_nodes, storage_context=storage_context)\n",
    "base_retriever = base_index.as_retriever(similarity_top_k=6)\n",
    "\n",
    "# Create auto-merging retriever\n",
    "auto_merging_retriever = AutoMergingRetriever(\n",
    "    base_retriever, \n",
    "    storage_context,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "query = DEMO_QUERIES[\"advanced\"]  # \"How do neural networks work in deep learning?\"\n",
    "nodes = auto_merging_retriever.retrieve(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Auto-merged to {len(nodes)} nodes\")\n",
    "for i, node in enumerate(nodes[:3], 1):\n",
    "    print(f\"{i}. Score: {node.score:.4f}\" if hasattr(node, 'score') and node.score else f\"{i}. (Auto-merged)\")\n",
    "    print(f\"   Text: {node.text[:120]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e977fc-0ccd-4d50-a2af-c21f280a3be0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Recursive Retriever â€“ Following Connections Across Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eefba6-0d76-4ea7-9cda-91d9a0cd74dc",
   "metadata": {},
   "source": [
    "\n",
    "The Recursive Retriever is built for scenarios where information is interconnected through explicit relationships or references.  \n",
    "Instead of treating each document or chunk in isolation, it actively follows links â€” such as citations, cross-references, or hierarchical pointers â€” to discover and retrieve related content across multiple pieces or layers.\n",
    "\n",
    "### Core Operating Principle\n",
    "\n",
    "The retriever works by:\n",
    "\n",
    "1. Starting with an initial query against a set of nodes\n",
    "2. Detecting any **reference links** stored in node metadata\n",
    "3. Automatically following those references to other nodes (or even other retrievers/query engines)\n",
    "4. Executing sub-queries as needed on the referenced content\n",
    "5. Combining results from the original matches + all followed references\n",
    "\n",
    "This creates a traversal mechanism that can jump between related pieces of information, building a more complete picture.\n",
    "\n",
    "### Types of References It Can Follow\n",
    "\n",
    "- **Structural / Hierarchical references**  \n",
    "  Example: small detail chunks pointing to their containing parent section or full document\n",
    "\n",
    "- **Citation-style references**  \n",
    "  Example: academic paper excerpts that cite other papers, sections, or figures\n",
    "\n",
    "- **Metadata-driven links**  \n",
    "  Example: a generated summary/question that points back to the full source content\n",
    "\n",
    "- **Cross-document relationships**  \n",
    "  Example: knowledge graphâ€“like connections between different reports, manuals, or database entries\n",
    "\n",
    "### Ideal Use Cases\n",
    "\n",
    "This retriever excels in environments where knowledge is naturally networked:\n",
    "\n",
    "- Academic literature and research papers with extensive citation networks\n",
    "- Technical documentation containing many internal/external cross-references\n",
    "- Legal corpora with references to statutes, precedents, and clauses\n",
    "- Enterprise knowledge bases with interconnected policies, procedures, and FAQs\n",
    "- Systems that include tables, figures, appendices, or supplementary materials that are referenced from the main text\n",
    "- Any domain where understanding requires following chains of related information\n",
    "\n",
    "### Typical Configuration Elements\n",
    "\n",
    "- `retriever_map` â€” dictionary connecting reference keys/IDs to specific retriever instances\n",
    "- `query_engine_map` â€” mapping that allows sub-queries to be routed to specialized query engines\n",
    "- Node metadata fields containing reference pointers (often as node IDs, file paths, or external keys)\n",
    "- Depth/recursion limits (to prevent runaway reference chasing)\n",
    "\n",
    "### Primary Strengths\n",
    "\n",
    "- Naturally handles interconnected knowledge structures\n",
    "- Enables multi-hop reasoning across documents and references\n",
    "- Retrieves more complete and contextual answers than isolated chunk retrieval\n",
    "- Can automatically discover relevant content users didnâ€™t explicitly mention\n",
    "- Particularly powerful for scholarly, technical, and regulatory domains\n",
    "\n",
    "### Main Limitations\n",
    "\n",
    "- Requires accurate, well-structured reference metadata to work effectively\n",
    "- Setup is significantly more involved than basic retrievers\n",
    "- Deep or very wide reference chains can become slow and resource-intensive\n",
    "- Risk of retrieving excessive or tangentially related content if limits arenâ€™t tuned\n",
    "- Debugging complex traversal paths can be challenging\n",
    "\n",
    "**In essence**:  \n",
    "The Recursive Retriever transforms a simple similarity search into an intelligent navigation system that follows meaningful connections â€” making it the preferred choice whenever your content contains rich relationships, citations, or layered references that should be explored together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "097c5f7d-f67f-4944-bd39-193ee8952238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "5. RECURSIVE RETRIEVER\n",
      "============================================================\n",
      "\u001b[1;3;34mRetrieving with query id None: What are the applications of AI?\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: Natural language processing enables computers to understand, interpret, and generate human language.\n",
      "\u001b[0mQuery: What are the applications of AI?\n",
      "Recursively retrieved 2 nodes\n",
      "1. Score: 0.6907\n",
      "   Text: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn fr...\n",
      "\n",
      "2. Score: 0.6500\n",
      "   Text: Natural language processing enables computers to understand, interpret, and generate human language....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"5. RECURSIVE RETRIEVER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create documents with references\n",
    "docs_with_refs = []\n",
    "for i, doc in enumerate(lab.documents):\n",
    "    # Add reference metadata\n",
    "    ref_doc = Document(\n",
    "        text=doc.text,\n",
    "        metadata={\n",
    "            \"doc_id\": f\"doc_{i}\",\n",
    "            \"references\": [f\"doc_{j}\" for j in range(len(lab.documents)) if j != i][:2]\n",
    "        }\n",
    "    )\n",
    "    docs_with_refs.append(ref_doc)\n",
    "\n",
    "# Create index with referenced documents\n",
    "ref_index = VectorStoreIndex.from_documents(docs_with_refs)\n",
    "\n",
    "# Create retriever mapping\n",
    "retriever_dict = {\n",
    "    f\"doc_{i}\": ref_index.as_retriever(similarity_top_k=1)\n",
    "    for i in range(len(docs_with_refs))\n",
    "}\n",
    "\n",
    "# Base retriever\n",
    "base_retriever = ref_index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "# Add the root retriever to the dictionary\n",
    "retriever_dict[\"vector\"] = base_retriever\n",
    "\n",
    "# Recursive retriever\n",
    "recursive_retriever = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict=retriever_dict,\n",
    "    query_engine_dict={},\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "query = DEMO_QUERIES[\"applications\"]  # \"What are the applications of AI?\"\n",
    "try:\n",
    "    nodes = recursive_retriever.retrieve(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Recursively retrieved {len(nodes)} nodes\")\n",
    "    for i, node in enumerate(nodes[:3], 1):\n",
    "        print(f\"{i}. Score: {node.score:.4f}\" if hasattr(node, 'score') and node.score else f\"{i}. (Recursive)\")\n",
    "        print(f\"   Text: {node.text[:100]}...\")\n",
    "        print()\n",
    "except Exception as e:\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Recursive retriever demo: {str(e)}\")\n",
    "    print(\"Note: Recursive retriever requires specific node reference setup\")\n",
    "    \n",
    "    # Fallback to basic retrieval for demonstration\n",
    "    print(\"\\nFalling back to basic retrieval demonstration...\")\n",
    "    base_nodes = base_retriever.retrieve(query)\n",
    "    for i, node in enumerate(base_nodes[:2], 1):\n",
    "        print(f\"{i}. Score: {node.score:.4f}\")\n",
    "        print(f\"   Text: {node.text[:100]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab3224d-c78d-4cae-9090-97261446dd37",
   "metadata": {},
   "source": [
    "## 6. Query Fusion Retriever â€“ Ensemble Retrieval with Smart Result Blending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f370f1-5179-4add-abf5-e68ebb9f1c77",
   "metadata": {},
   "source": [
    "\n",
    "The Query Fusion Retriever (often implemented via `QueryFusionRetriever` in LlamaIndex) is an advanced ensemble technique that combines strengths from multiple retrieval approaches and/or multiple reformulations of the original query.  \n",
    "It boosts overall recall and robustness by intelligently merging results using proven fusion algorithms.\n",
    "\n",
    "### How It Operates\n",
    "\n",
    "1. **Multi-Retriever Ensemble**  \n",
    "   Takes a list of different retrievers (e.g., dense vector + BM25 keyword, or vector retrievers from different indexes/chunk sizes) and runs the query (or queries) against each.\n",
    "\n",
    "2. **Optional Query Expansion**  \n",
    "   Uses an LLM to automatically generate several alternative phrasings or sub-queries of the original question (e.g., 3â€“4 variations).  \n",
    "   This helps when the userâ€™s wording doesnâ€™t perfectly match the stored content.\n",
    "\n",
    "3. **Result Fusion**  \n",
    "   Collects nodes from all retriever Ã— query combinations  \n",
    "   Applies a fusion/reranking strategy to produce a single, re-ranked list of the most relevant nodes  \n",
    "   Handles deduplication automatically\n",
    "\n",
    "### Main Fusion Strategies Available\n",
    "\n",
    "- **Reciprocal Rank Fusion (RRF)**  \n",
    "  â†’ Classic, score-independent method: sums 1/(rank + constant) across all result lists  \n",
    "  â†’ Very robust, ignores absolute score differences, focuses purely on relative positions  \n",
    "  â†’ Most popular default choice\n",
    "\n",
    "- **Relative Score Fusion**  \n",
    "  â†’ Normalizes scores within each retriever/result-set (e.g., Min-Max scaling)  \n",
    "  â†’ Then performs a weighted combination  \n",
    "  â†’ Preserves confidence differences within each method\n",
    "\n",
    "- **Distribution-Based Score Fusion**  \n",
    "  â†’ Normalizes using statistical properties (mean + standard deviation) of each result set  \n",
    "  â†’ Helps when retrievers produce scores on very different scales\n",
    "\n",
    "### Ideal Scenarios\n",
    "\n",
    "Use Query Fusion when you want:\n",
    "\n",
    "- Best of both worlds: semantic understanding + exact keyword precision\n",
    "- Robustness against poorly phrased or ambiguous user questions\n",
    "- Higher recall for complex, multi-faceted, or exploratory queries\n",
    "- Combining different indexes (e.g., small vs large chunks, different embedding models)\n",
    "- General-purpose Q&A systems where query formulation varies widely\n",
    "\n",
    "### Typical Configuration Options\n",
    "\n",
    "- List of retrievers to combine\n",
    "- `num_queries` â€” how many query variations to generate (set to 1 to disable LLM rewriting)\n",
    "- `similarity_top_k` â€” final number of nodes to return after fusion\n",
    "- `mode` â€” fusion algorithm (`reciprocal_rerank`, `relative_score`, `dist_based_score`)\n",
    "- `use_async` â€” enable parallel execution for speed\n",
    "- Optional custom prompt for query generation\n",
    "\n",
    "### Primary Advantages\n",
    "\n",
    "- Significantly improves recall without sacrificing much precision\n",
    "- Reduces sensitivity to exact query wording\n",
    "- Leverages complementary strengths of different retrieval methods\n",
    "- Automatically handles node deduplication across sources\n",
    "- Flexible â€” works with any combination of retrievers\n",
    "\n",
    "### Key Drawbacks\n",
    "\n",
    "- Higher latency and compute cost (multiple retrievals + possible LLM calls)\n",
    "- Additional expense if using LLM for query variations\n",
    "- Can occasionally introduce less relevant noise if fusion isnâ€™t perfectly tuned\n",
    "- More complex to set up, debug, and explain compared to single-retriever approaches\n",
    "\n",
    "**Bottom line**:  \n",
    "The Query Fusion Retriever is one of the most powerful \"plug-and-play\" ways to get state-of-the-art retrieval performance in real-world applications â€” especially when combining semantic vector search with keyword precision and protecting against variable user phrasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65cae658-b313-4c6f-a941-90b6df97f05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "6. QUERY FUSION RETRIEVER - OVERVIEW\n",
      "============================================================\n",
      "Query: What are the main approaches to machine learning?\n",
      "QueryFusionRetriever generates multiple query variations and fuses results\n",
      "using one of three sophisticated fusion modes.\n",
      "\n",
      "Overview of Fusion Modes:\n",
      "1. RECIPROCAL_RERANK: Uses reciprocal rank fusion (most robust)\n",
      "2. RELATIVE_SCORE: Preserves score magnitudes (most interpretable)\n",
      "3. DIST_BASED_SCORE: Statistical normalization (most sophisticated)\n",
      "\n",
      "Demonstration workflow:\n",
      "Each subsection below explores one fusion mode in detail with:\n",
      "- Theoretical explanation of the fusion method\n",
      "- Live demonstration using QueryFusionRetriever\n",
      "- Manual implementation showing the underlying mathematics\n",
      "- Use case recommendations and trade-offs\n",
      "\n",
      "Using consistent test query throughout: 'What are the main approaches to machine learning?'\n",
      "This allows direct comparison of how each fusion mode handles the same input.\n",
      "\n",
      "Proceed to subsections 6.1, 6.2, and 6.3 for detailed demonstrations...\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"6. QUERY FUSION RETRIEVER - OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create base retriever\n",
    "base_retriever = lab.vector_index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "query = DEMO_QUERIES[\"comprehensive\"]  # \"What are the main approaches to machine learning?\"\n",
    "print(f\"Query: {query}\")\n",
    "print(\"QueryFusionRetriever generates multiple query variations and fuses results\")\n",
    "print(\"using one of three sophisticated fusion modes.\")\n",
    "\n",
    "print(\"\\nOverview of Fusion Modes:\")\n",
    "print(\"1. RECIPROCAL_RERANK: Uses reciprocal rank fusion (most robust)\")\n",
    "print(\"2. RELATIVE_SCORE: Preserves score magnitudes (most interpretable)\")  \n",
    "print(\"3. DIST_BASED_SCORE: Statistical normalization (most sophisticated)\")\n",
    "\n",
    "print(\"\\nDemonstration workflow:\")\n",
    "print(\"Each subsection below explores one fusion mode in detail with:\")\n",
    "print(\"- Theoretical explanation of the fusion method\")\n",
    "print(\"- Live demonstration using QueryFusionRetriever\")\n",
    "print(\"- Manual implementation showing the underlying mathematics\")\n",
    "print(\"- Use case recommendations and trade-offs\")\n",
    "\n",
    "print(f\"\\nUsing consistent test query throughout: '{query}'\")\n",
    "print(\"This allows direct comparison of how each fusion mode handles the same input.\")\n",
    "\n",
    "print(\"\\nProceed to subsections 6.1, 6.2, and 6.3 for detailed demonstrations...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a836941e-fb99-477b-be5a-6f18d1d23196",
   "metadata": {},
   "source": [
    "### 6.1 Reciprocal Rank Fusion (RRF) â€“ The Most Reliable Fusion Approach\n",
    "\n",
    "Reciprocal Rank Fusion (RRF) is widely regarded as the most robust and dependable method for combining ranked lists in the QueryFusionRetriever.  \n",
    "It intelligently merges results from multiple query variations (or multiple retrievers) by focusing purely on **relative ranking positions** rather than raw similarity scores.\n",
    "\n",
    "### How RRF Works in Practice\n",
    "\n",
    "1. **Multiple query perspectives**  \n",
    "   The system creates several reformulated versions of the original question  \n",
    "   Example: \"What are main machine learning methods?\" â†’ \"ML algorithms overview\", \"core techniques in machine learning\", \"approaches to learning from data\"\n",
    "\n",
    "2. **Independent retrieval**  \n",
    "   Each query variation is sent to the retriever(s), producing its own ranked list of results\n",
    "\n",
    "3. **Reciprocal rank scoring**  \n",
    "   For every document that appears in any of the lists:  \n",
    "   â†’ Take its position (rank) in each list  \n",
    "   â†’ Compute: `1 / (rank + k)`  \n",
    "   â†’ Sum these values across all lists where the document appeared\n",
    "\n",
    "4. **Final ranking**  \n",
    "   Documents are sorted by their total summed reciprocal rank score (highest first)\n",
    "\n",
    "**Core mathematical expression**:\n",
    "```\n",
    "RRF_score(d) = Î£ (1 / (rank_i(d) + k))\n",
    "```\n",
    "- `d` = document  \n",
    "- `rank_i(d)` = position of document d in the ranked list of query variation i  \n",
    "- `k` = constant (standard value = 60) that prevents division by zero and smooths the influence of very high ranks\n",
    "\n",
    "### Why RRF Performs So Well for Fusion\n",
    "\n",
    "- **Completely ignores score scales** â€” works even when different retrievers or queries produce wildly different numerical score ranges  \n",
    "- **Downplays outliers** â€” a document that ranks #1 in one list but #200 in another wonâ€™t dominate unfairly  \n",
    "- **Treats ranking position as the primary signal** â€” which is often more reliable than absolute similarity values  \n",
    "- **Very stable across different query phrasings** â€” reduces sensitivity to how the user originally worded the question  \n",
    "- **Strong track record** â€” one of the most battle-tested fusion methods in modern information retrieval\n",
    "\n",
    "### Best Situations to Choose RRF Mode\n",
    "\n",
    "RRF is typically the **go-to / default** fusion strategy when:\n",
    "\n",
    "- You are using multiple query reformulations\n",
    "- Different retrievers produce results on very different scales\n",
    "- You want maximum stability and consistency in production\n",
    "- Query phrasing varies widely between users\n",
    "- You prefer a method that requires almost no tuning\n",
    "\n",
    "### Key Strengths\n",
    "\n",
    "- Extremely robust and predictable behavior\n",
    "- No need to normalize scores or tune complex weights\n",
    "- Gracefully handles cases where some query variations return fewer results\n",
    "- Computationally lightweight compared to score-normalization approaches\n",
    "- Rarely produces pathological results\n",
    "\n",
    "### Main Trade-offs\n",
    "\n",
    "- Discards absolute confidence information (only cares about order)\n",
    "- Gives equal influence to every query variation (no built-in weighting)\n",
    "- May under-emphasize documents that have very strong scores in one view but mediocre ranks in others\n",
    "- Less sophisticated than methods that preserve score magnitude when all retrievers are very well calibrated\n",
    "\n",
    "**In summary**:  \n",
    "Reciprocal Rank Fusion is the safest, most reliable, and most widely recommended way to combine results in a Query Fusion Retriever â€” especially when you want consistent performance with minimal tuning and maximum tolerance to variation in query formulation or retriever behavior.\n",
    "\n",
    "*Based on: https://docs.llamaindex.ai/en/stable/examples/retrievers/reciprocal_rerank_fusion/*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4838bea-eb6e-4976-b8a4-cde6c1e35b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "6.1 RECIPROCAL RANK FUSION MODE DEMONSTRATION\n",
      "============================================================\n",
      "Testing QueryFusionRetriever with reciprocal_rerank mode:\n",
      "This demonstrates how RRF works within the query fusion framework\n",
      "\n",
      "Query: What are the main approaches to machine learning?\n",
      "QueryFusionRetriever will:\n",
      "1. Generate query variations using LLM\n",
      "2. Retrieve results for each variation\n",
      "3. Apply Reciprocal Rank Fusion\n",
      "Generated queries:\n",
      "Here are two search queries related to the input query:\n",
      "1. \"Machine learning methodologies and techniques\"\n",
      "2. \"Approaches to machine learning in artificial intelligence\"\n",
      "\n",
      "RRF Query Fusion Results:\n",
      "1. Final RRF Score: 0.0492\n",
      "   Text: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn fr...\n",
      "\n",
      "2. Final RRF Score: 0.0487\n",
      "   Text: Deep learning uses neural networks with multiple layers to model and understand complex patterns in ...\n",
      "\n",
      "3. Final RRF Score: 0.0484\n",
      "   Text: Supervised learning uses labeled training data to learn a mapping from inputs to outputs....\n",
      "\n",
      "RRF Benefits in Query Fusion Context:\n",
      "- Automatically handles query variations of different quality\n",
      "- No bias toward queries that return higher raw scores\n",
      "- Stable performance across diverse query formulations\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"6.1 RECIPROCAL RANK FUSION MODE DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create QueryFusionRetriever with RRF mode\n",
    "base_retriever = lab.vector_index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "print(\"Testing QueryFusionRetriever with reciprocal_rerank mode:\")\n",
    "print(\"This demonstrates how RRF works within the query fusion framework\")\n",
    "\n",
    "# Use the same query for consistency across all fusion modes\n",
    "query = DEMO_QUERIES[\"comprehensive\"]  # \"What are the main approaches to machine learning?\"\n",
    "\n",
    "try:\n",
    "    # Create query fusion retriever with RRF mode\n",
    "    rrf_query_fusion = QueryFusionRetriever(\n",
    "        [base_retriever],\n",
    "        similarity_top_k=3,\n",
    "        num_queries=3,\n",
    "        mode=\"reciprocal_rerank\",\n",
    "        use_async=False,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"QueryFusionRetriever will:\")\n",
    "    print(\"1. Generate query variations using LLM\")\n",
    "    print(\"2. Retrieve results for each variation\")\n",
    "    print(\"3. Apply Reciprocal Rank Fusion\")\n",
    "    \n",
    "    nodes = rrf_query_fusion.retrieve(query)\n",
    "    \n",
    "    print(f\"\\nRRF Query Fusion Results:\")\n",
    "    for i, node in enumerate(nodes, 1):\n",
    "        print(f\"{i}. Final RRF Score: {node.score:.4f}\")\n",
    "        print(f\"   Text: {node.text[:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"RRF Benefits in Query Fusion Context:\")\n",
    "    print(\"- Automatically handles query variations of different quality\")\n",
    "    print(\"- No bias toward queries that return higher raw scores\")\n",
    "    print(\"- Stable performance across diverse query formulations\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"QueryFusionRetriever error: {e}\")\n",
    "    print(\"Demonstrating RRF concept manually with query variations...\")\n",
    "    \n",
    "    # Manual demonstration with query variations derived from the main query\n",
    "    query_variations = [\n",
    "        DEMO_QUERIES[\"comprehensive\"],  # Original query\n",
    "        \"machine learning approaches and methods\",\n",
    "        \"different ML techniques and algorithms\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Manual RRF with Query Variations:\")\n",
    "    all_results = {}\n",
    "    \n",
    "    for i, query_var in enumerate(query_variations):\n",
    "        print(f\"\\nQuery variation {i+1}: {query_var}\")\n",
    "        nodes = base_retriever.retrieve(query_var)\n",
    "        \n",
    "        # Apply RRF scoring\n",
    "        for rank, node in enumerate(nodes):\n",
    "            node_id = node.node.node_id\n",
    "            if node_id not in all_results:\n",
    "                all_results[node_id] = {\n",
    "                    'node': node,\n",
    "                    'rrf_score': 0,\n",
    "                    'query_ranks': []\n",
    "                }\n",
    "            \n",
    "            # Calculate RRF contribution: 1 / (rank + k)\n",
    "            k = 60  # Standard RRF parameter\n",
    "            rrf_contribution = 1.0 / (rank + 1 + k)\n",
    "            all_results[node_id]['rrf_score'] += rrf_contribution\n",
    "            all_results[node_id]['query_ranks'].append((i, rank + 1))\n",
    "    \n",
    "    # Sort by final RRF score\n",
    "    sorted_results = sorted(\n",
    "        all_results.values(), \n",
    "        key=lambda x: x['rrf_score'], \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCombined RRF Results (top 3):\")\n",
    "    for i, result in enumerate(sorted_results[:3], 1):\n",
    "        print(f\"{i}. Final RRF Score: {result['rrf_score']:.4f}\")\n",
    "        print(f\"   Query ranks: {result['query_ranks']}\")\n",
    "        print(f\"   Text: {result['node'].text[:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"RRF Formula Demonstration:\")\n",
    "    print(\"For each document: RRF_score = Î£(1 / (rank + 60))\")\n",
    "    print(\"- Rank 1 in query: 1/(1+60) = 0.0164\")\n",
    "    print(\"- Rank 2 in query: 1/(2+60) = 0.0161\")\n",
    "    print(\"- Rank 3 in query: 1/(3+60) = 0.0159\")\n",
    "    print(\"Documents appearing in multiple queries get higher combined scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1860ca5-7efe-40a1-a238-479954f6cd3b",
   "metadata": {},
   "source": [
    "### 6.2 Relative Score Fusion Mode â€“ Score-Aware Result Blending\n",
    "\n",
    "Relative Score Fusion is a sophisticated fusion strategy in the `QueryFusionRetriever` that goes beyond pure ranking positions by **incorporating the actual similarity/confidence scores** produced by each retriever or query variation.  \n",
    "It normalizes those scores within each result set and then combines them â€” making it particularly useful when the magnitude of scores carries meaningful information.\n",
    "\n",
    "### How Relative Score Fusion Operates\n",
    "\n",
    "1. **Multiple perspectives (optional)**  \n",
    "   If enabled, an LLM generates several rephrased versions of the original query.\n",
    "\n",
    "2. **Independent retrieval**  \n",
    "   Each retriever (or each query variation) returns its top results along with their raw similarity scores.\n",
    "\n",
    "3. **Per-set normalization**  \n",
    "   For every individual result list (one per retriever or per query variation):  \n",
    "   - Apply **Min-Max scaling** to transform the raw scores into a consistent range, usually [0, 1]  \n",
    "   - The highest-scoring result in that list gets normalized to 1.0  \n",
    "   - All others are scaled proportionally relative to that maximum\n",
    "\n",
    "4. **Score combination**  \n",
    "   For each candidate node:  \n",
    "   - Sum (or weighted sum) its normalized scores across all lists where it appeared  \n",
    "   - Final ranking is based on this combined normalized score\n",
    "\n",
    "**Simplified mathematical view**:\n",
    "```\n",
    "normalized_score_i(d) = score_i(d) / max_score_i\n",
    "combined_score(d) = Î£ (weight_i Ã— normalized_score_i(d))\n",
    "```\n",
    "(where weights are equal by default, or can be customized via `retriever_weights`)\n",
    "\n",
    "### Why Choose Relative Score Fusion?\n",
    "\n",
    "- **Keeps score confidence information** â€” unlike RRF, it respects how strongly each retriever/query believed in its top results  \n",
    "- **Handles different scoring scales fairly** â€” normalization makes scores from vector search, BM25, etc., comparable  \n",
    "- **More nuanced ranking** â€” documents with very high confidence in one view can stand out more than in pure rank-based methods  \n",
    "- **Customizable influence** â€” you can assign different weights to different retrievers (e.g., favor vector over keyword search)\n",
    "\n",
    "### Best Situations to Select Relative Score Mode\n",
    "\n",
    "Use this fusion method when:\n",
    "\n",
    "- Combining retrievers that produce meaningful similarity scores (dense embeddings + sparse keyword, etc.)\n",
    "- You want the strength of matches (not just order) to influence final ranking\n",
    "- Different retrieval methods have very different raw score ranges\n",
    "- You need more interpretable fusion where score magnitude matters\n",
    "- Working in hybrid search setups (vector + keyword) where confidence should carry weight\n",
    "\n",
    "### Configuration in QueryFusionRetriever\n",
    "\n",
    "- `mode=\"relative_score\"` â†’ activates this fusion algorithm  \n",
    "- `retriever_weights` â€” list of floats summing to 1 (e.g. `[0.7, 0.3]` for vector-heavy bias)  \n",
    "- `similarity_top_k` â€” final number of nodes after fusion  \n",
    "- `num_queries` â€” set >1 to enable LLM query variations (set to 1 to disable)  \n",
    "- `use_async=True` â€” recommended for performance\n",
    "\n",
    "### Primary Advantages\n",
    "\n",
    "- Captures relative confidence within each retrieval method  \n",
    "- Produces intuitive, score-proportional final rankings  \n",
    "- Excellent for hybrid retrieval (semantic + lexical)  \n",
    "- Allows explicit control over retriever importance via weights  \n",
    "- Generally more precise than rank-only methods when scores are reliable\n",
    "\n",
    "### Main Limitations\n",
    "\n",
    "- Sensitive to outliers â€” an abnormally high raw score in one list can skew normalization  \n",
    "- Assumes raw scores are meaningful and comparable after normalization  \n",
    "- Less robust than RRF when score scales are wildly inconsistent or noisy  \n",
    "- Slightly more computation than pure reciprocal rank approaches\n",
    "\n",
    "**Bottom line**:  \n",
    "Relative Score Fusion is the preferred choice when you want to leverage **both ranking order and score magnitude** for more accurate and confidence-aware result blending â€” especially in hybrid search setups where different retrievers provide complementary signals with interpretable scores.\n",
    "\n",
    "*Based on: https://docs.llamaindex.ai/en/stable/examples/retrievers/relative_score_dist_fusion/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "036cb879-314a-439a-b3aa-7f0c15aba1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "6.2 RELATIVE SCORE FUSION MODE DEMONSTRATION\n",
      "============================================================\n",
      "Testing QueryFusionRetriever with relative_score mode:\n",
      "This mode preserves score magnitudes while normalizing across query variations\n",
      "\n",
      "Query: What are the main approaches to machine learning?\n",
      "QueryFusionRetriever with relative_score will:\n",
      "1. Generate query variations\n",
      "2. Normalize scores within each variation (score/max_score)\n",
      "3. Combine normalized scores\n",
      "\n",
      "Relative Score Fusion Results:\n",
      "1. Combined Relative Score: 1.0000\n",
      "   Text: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn fr...\n",
      "\n",
      "2. Combined Relative Score: 0.3905\n",
      "   Text: Supervised learning uses labeled training data to learn a mapping from inputs to outputs....\n",
      "\n",
      "3. Combined Relative Score: 0.3317\n",
      "   Text: Deep learning uses neural networks with multiple layers to model and understand complex patterns in ...\n",
      "\n",
      "Relative Score Benefits in Query Fusion:\n",
      "- Preserves confidence information from embedding model\n",
      "- Ensures fair contribution from each query variation\n",
      "- More interpretable than rank-only methods\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"6.2 RELATIVE SCORE FUSION MODE DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "base_retriever = lab.vector_index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "print(\"Testing QueryFusionRetriever with relative_score mode:\")\n",
    "print(\"This mode preserves score magnitudes while normalizing across query variations\")\n",
    "\n",
    "# Use the same query for consistency across all fusion modes\n",
    "query = DEMO_QUERIES[\"comprehensive\"]  # \"What are the main approaches to machine learning?\"\n",
    "\n",
    "try:\n",
    "    # Create query fusion retriever with relative score mode\n",
    "    rel_score_fusion = QueryFusionRetriever(\n",
    "        [base_retriever],\n",
    "        similarity_top_k=3,\n",
    "        num_queries=3,\n",
    "        mode=\"relative_score\",\n",
    "        use_async=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"QueryFusionRetriever with relative_score will:\")\n",
    "    print(\"1. Generate query variations\")\n",
    "    print(\"2. Normalize scores within each variation (score/max_score)\")\n",
    "    print(\"3. Combine normalized scores\")\n",
    "    \n",
    "    nodes = rel_score_fusion.retrieve(query)\n",
    "    \n",
    "    print(f\"\\nRelative Score Fusion Results:\")\n",
    "    for i, node in enumerate(nodes, 1):\n",
    "        print(f\"{i}. Combined Relative Score: {node.score:.4f}\")\n",
    "        print(f\"   Text: {node.text[:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"Relative Score Benefits in Query Fusion:\")\n",
    "    print(\"- Preserves confidence information from embedding model\")\n",
    "    print(\"- Ensures fair contribution from each query variation\")\n",
    "    print(\"- More interpretable than rank-only methods\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"QueryFusionRetriever error: {e}\")\n",
    "    print(\"Demonstrating Relative Score concept manually...\")\n",
    "    \n",
    "    # Manual demonstration with query variations derived from the main query\n",
    "    query_variations = [\n",
    "        DEMO_QUERIES[\"comprehensive\"],  # Original query\n",
    "        \"machine learning approaches and methods\",\n",
    "        \"different ML techniques and algorithms\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Manual Relative Score Fusion with Query Variations:\")\n",
    "    all_results = {}\n",
    "    query_max_scores = []\n",
    "    \n",
    "    # Step 1: Get results and find max scores for each query\n",
    "    for i, query_var in enumerate(query_variations):\n",
    "        print(f\"\\nQuery variation {i+1}: {query_var}\")\n",
    "        nodes = base_retriever.retrieve(query_var)\n",
    "        scores = [node.score or 0 for node in nodes]\n",
    "        max_score = max(scores) if scores else 1.0\n",
    "        query_max_scores.append(max_score)\n",
    "        \n",
    "        print(f\"Max score for this query: {max_score:.4f}\")\n",
    "        \n",
    "        # Store results with normalization info\n",
    "        for node in nodes:\n",
    "            node_id = node.node.node_id\n",
    "            original_score = node.score or 0\n",
    "            normalized_score = original_score / max_score if max_score > 0 else 0\n",
    "            \n",
    "            if node_id not in all_results:\n",
    "                all_results[node_id] = {\n",
    "                    'node': node,\n",
    "                    'combined_score': 0,\n",
    "                    'contributions': []\n",
    "                }\n",
    "            \n",
    "            all_results[node_id]['combined_score'] += normalized_score\n",
    "            all_results[node_id]['contributions'].append({\n",
    "                'query': i,\n",
    "                'original': original_score,\n",
    "                'normalized': normalized_score\n",
    "            })\n",
    "    \n",
    "    # Step 2: Sort by combined relative score\n",
    "    sorted_results = sorted(\n",
    "        all_results.values(),\n",
    "        key=lambda x: x['combined_score'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCombined Relative Score Results (top 3):\")\n",
    "    for i, result in enumerate(sorted_results[:3], 1):\n",
    "        print(f\"{i}. Combined Score: {result['combined_score']:.4f}\")\n",
    "        print(f\"   Score breakdown:\")\n",
    "        for contrib in result['contributions']:\n",
    "            print(f\"     Query {contrib['query']}: {contrib['original']:.3f} â†’ {contrib['normalized']:.3f}\")\n",
    "        print(f\"   Text: {result['node'].text[:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"Relative Score Normalization Process:\")\n",
    "    print(\"1. For each query variation, find max_score\")\n",
    "    print(\"2. Normalize: normalized_score = original_score / max_score\")\n",
    "    print(\"3. Sum normalized scores across all query variations\")\n",
    "    print(\"4. Documents with consistently high scores across queries win\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc5a4f-d4f6-426b-bc75-6fed35c62d96",
   "metadata": {},
   "source": [
    "### 6.3 Distribution-Based Score Fusion Mode â€“ Statistically Sophisticated Result Combination\n",
    "\n",
    "Distribution-Based Score Fusion represents the most statistically aware approach available in the `QueryFusionRetriever`.  \n",
    "It goes beyond simple rank-based or min-max normalization by actively analyzing the **shape and characteristics** of the score distribution produced by each query variation or retriever, then using that information for smarter, more robust score normalization and combination.\n",
    "\n",
    "### How Distribution-Based Fusion Works\n",
    "\n",
    "1. **Multi-perspective retrieval**  \n",
    "   (Optional) LLM generates several rephrased versions of the query  \n",
    "   Each version (or each retriever) produces its own ranked list with raw similarity scores\n",
    "\n",
    "2. **Distribution analysis per result set**  \n",
    "   For every individual result list, the system computes key statistical properties:  \n",
    "   - Mean and standard deviation  \n",
    "   - Median, quartiles, interquartile range (IQR)  \n",
    "   - Minimum/maximum values, skewness, etc.\n",
    "\n",
    "3. **Advanced normalization techniques**  \n",
    "   Common methods applied include:  \n",
    "   - **Z-score normalization** â†’ `(score - mean) / std_dev`  \n",
    "     Often followed by a sigmoid transformation to map into a stable [0,1] range  \n",
    "   - **Percentile-based scaling** â†’ converting each score to its percentile rank within its own list  \n",
    "   - **Robust scaling using IQR** â†’ especially helpful when distributions contain outliers  \n",
    "   - **Winsorization** or clipping of extreme values (in some implementations)\n",
    "\n",
    "4. **Confidence-aware combination**  \n",
    "   Normalized scores are then aggregated, often with:  \n",
    "   - Weights influenced by distribution characteristics (e.g., tighter distributions = higher confidence)  \n",
    "   - Adaptive blending that respects how reliable each query variation appears statistically\n",
    "\n",
    "### Why This Approach Stands Out\n",
    "\n",
    "- **Respects real distribution shapes** â€” handles cases where one query variation produces tight, confident scores while another is scattered  \n",
    "- **Excellent outlier resistance** â€” statistical methods naturally dampen the effect of anomalous high/low scores  \n",
    "- **Adaptive behavior** â€” can give more influence to query variations that show more consistent, trustworthy score patterns  \n",
    "- **Works with multi-modal / heterogeneous scoring** â€” when different retrievers or query phrasings produce fundamentally different score behaviors\n",
    "\n",
    "### Best Situations to Choose Distribution-Based Mode\n",
    "\n",
    "Use this fusion strategy when:\n",
    "\n",
    "- Different query reformulations or retrievers generate dramatically different score distributions  \n",
    "- Some query variations are expected to be much more reliable than others  \n",
    "- Youâ€™re dealing with noisy retrieval systems or inconsistent scoring mechanisms  \n",
    "- You want the most statistically principled way to combine potentially unreliable signals  \n",
    "- Working in research-grade or high-stakes retrieval applications where robustness matters most\n",
    "\n",
    "### Configuration in QueryFusionRetriever\n",
    "\n",
    "- `mode=\"dist_based_score\"` (or equivalent identifier in current implementation)  \n",
    "- Usually automatic distribution analysis and robust scaling  \n",
    "- Optional parameters for: outlier thresholds, minimum result count for reliable stats, confidence weighting factors  \n",
    "- Works best when each result list contains enough documents (typically â‰¥10â€“20)\n",
    "\n",
    "### Primary Advantages\n",
    "\n",
    "- Most mathematically rigorous and theoretically sound fusion method  \n",
    "- Superior handling of score variability, noise, and outliers  \n",
    "- Naturally adapts to differences between query variations/retrievers  \n",
    "- Provides the strongest protection against pathological scoring behavior  \n",
    "- Especially valuable in hybrid or multi-retriever setups\n",
    "\n",
    "### Main Drawbacks\n",
    "\n",
    "- Highest computational cost among the fusion methods  \n",
    "- Needs sufficient results per query variation to estimate distributions reliably  \n",
    "- Can be overkill (and slower) for simple or well-behaved scoring scenarios  \n",
    "- Results are harder to interpret intuitively compared to RRF or relative score methods  \n",
    "- Requires careful tuning in edge cases with very small result sets\n",
    "\n",
    "**In summary**:  \n",
    "Distribution-Based Score Fusion is the most advanced and statistically sophisticated option in the QueryFusionRetriever toolkit â€” ideal when you need maximum robustness against wildly varying score distributions and want the fusion process to intelligently respect the statistical reliability of each retrieval perspective.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9aaad68-b2ec-4ca6-9b75-db8f13312907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "6.3 DISTRIBUTION-BASED SCORE FUSION MODE DEMONSTRATION\n",
      "============================================================\n",
      "Testing QueryFusionRetriever with dist_based_score mode:\n",
      "This mode uses statistical analysis for the most sophisticated score fusion\n",
      "\n",
      "Query: What are the main approaches to machine learning?\n",
      "QueryFusionRetriever with dist_based_score will:\n",
      "1. Generate query variations\n",
      "2. Analyze score distributions for each variation\n",
      "3. Apply statistical normalization (z-score, percentiles)\n",
      "4. Combine with distribution-aware weighting\n",
      "\n",
      "Distribution-Based Fusion Results:\n",
      "1. Statistically Normalized Score: 0.8696\n",
      "   Text: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn fr...\n",
      "\n",
      "2. Statistically Normalized Score: 0.5908\n",
      "   Text: Supervised learning uses labeled training data to learn a mapping from inputs to outputs....\n",
      "\n",
      "3. Statistically Normalized Score: 0.5646\n",
      "   Text: Deep learning uses neural networks with multiple layers to model and understand complex patterns in ...\n",
      "\n",
      "Distribution-Based Benefits in Query Fusion:\n",
      "- Accounts for score distribution differences between query variations\n",
      "- Statistically robust against outliers and noise\n",
      "- Adapts weighting based on query variation reliability\n",
      "\n",
      "============================================================\n",
      "FUSION MODES COMPARISON SUMMARY\n",
      "============================================================\n",
      "All three modes tested with the same query for direct comparison:\n",
      "Query: What are the main approaches to machine learning?\n",
      "\n",
      "Mode Characteristics:\n",
      "â€¢ RRF (reciprocal_rerank): Most robust, rank-based, scale-invariant\n",
      "â€¢ Relative Score: Preserves confidence, normalizes by max score\n",
      "â€¢ Distribution-Based: Most sophisticated, statistical normalization\n",
      "\n",
      "Choose based on your use case:\n",
      "- Production stability â†’ RRF\n",
      "- Score interpretability â†’ Relative Score\n",
      "- Statistical robustness â†’ Distribution-Based\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"6.3 DISTRIBUTION-BASED SCORE FUSION MODE DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "base_retriever = lab.vector_index.as_retriever(similarity_top_k=8)\n",
    "\n",
    "print(\"Testing QueryFusionRetriever with dist_based_score mode:\")\n",
    "print(\"This mode uses statistical analysis for the most sophisticated score fusion\")\n",
    "\n",
    "# Use the same query for consistency across all fusion modes\n",
    "query = DEMO_QUERIES[\"comprehensive\"]  # \"What are the main approaches to machine learning?\"\n",
    "\n",
    "try:\n",
    "    # Create query fusion retriever with distribution-based mode\n",
    "    dist_fusion = QueryFusionRetriever(\n",
    "        [base_retriever],\n",
    "        similarity_top_k=3,\n",
    "        num_queries=3,\n",
    "        mode=\"dist_based_score\",\n",
    "        use_async=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"QueryFusionRetriever with dist_based_score will:\")\n",
    "    print(\"1. Generate query variations\")\n",
    "    print(\"2. Analyze score distributions for each variation\")\n",
    "    print(\"3. Apply statistical normalization (z-score, percentiles)\")\n",
    "    print(\"4. Combine with distribution-aware weighting\")\n",
    "    \n",
    "    nodes = dist_fusion.retrieve(query)\n",
    "    \n",
    "    print(f\"\\nDistribution-Based Fusion Results:\")\n",
    "    for i, node in enumerate(nodes, 1):\n",
    "        print(f\"{i}. Statistically Normalized Score: {node.score:.4f}\")\n",
    "        print(f\"   Text: {node.text[:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"Distribution-Based Benefits in Query Fusion:\")\n",
    "    print(\"- Accounts for score distribution differences between query variations\")\n",
    "    print(\"- Statistically robust against outliers and noise\")\n",
    "    print(\"- Adapts weighting based on query variation reliability\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"QueryFusionRetriever error: {e}\")\n",
    "    print(\"Demonstrating Distribution-Based concept manually...\")\n",
    "    \n",
    "    if not SCIPY_AVAILABLE:\n",
    "        print(\"âš ï¸ Full statistical analysis requires scipy\")\n",
    "    \n",
    "    # Manual demonstration with query variations derived from the main query\n",
    "    query_variations = [\n",
    "        DEMO_QUERIES[\"comprehensive\"],  # Original query\n",
    "        \"machine learning approaches and methods\",\n",
    "        \"different ML techniques and algorithms\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Manual Distribution-Based Fusion with Query Variations:\")\n",
    "    all_results = {}\n",
    "    variation_stats = []\n",
    "    \n",
    "    # Step 1: Collect results and analyze distributions\n",
    "    for i, query_var in enumerate(query_variations):\n",
    "        print(f\"\\nQuery variation {i+1}: {query_var}\")\n",
    "        nodes = base_retriever.retrieve(query_var)\n",
    "        scores = [node.score or 0 for node in nodes]\n",
    "        \n",
    "        # Calculate distribution statistics\n",
    "        mean_score = np.mean(scores) if scores else 0\n",
    "        std_score = np.std(scores) if len(scores) > 1 else 1\n",
    "        min_score = np.min(scores) if scores else 0\n",
    "        max_score = np.max(scores) if scores else 1\n",
    "        \n",
    "        stats_info = {\n",
    "            'mean': mean_score,\n",
    "            'std': std_score,\n",
    "            'min': min_score,\n",
    "            'max': max_score,\n",
    "            'nodes': nodes,\n",
    "            'scores': scores\n",
    "        }\n",
    "        variation_stats.append(stats_info)\n",
    "        \n",
    "        print(f\"Distribution stats: mean={mean_score:.3f}, std={std_score:.3f}\")\n",
    "        print(f\"Score range: [{min_score:.3f}, {max_score:.3f}]\")\n",
    "        \n",
    "        # Apply z-score normalization\n",
    "        for node, score in zip(nodes, scores):\n",
    "            node_id = node.node.node_id\n",
    "            \n",
    "            # Z-score normalization\n",
    "            if std_score > 0:\n",
    "                z_score = (score - mean_score) / std_score\n",
    "            else:\n",
    "                z_score = 0\n",
    "            \n",
    "            # Convert to [0,1] using sigmoid\n",
    "            normalized_score = 1 / (1 + np.exp(-z_score))\n",
    "            \n",
    "            if node_id not in all_results:\n",
    "                all_results[node_id] = {\n",
    "                    'node': node,\n",
    "                    'combined_score': 0,\n",
    "                    'contributions': []\n",
    "                }\n",
    "            \n",
    "            all_results[node_id]['combined_score'] += normalized_score\n",
    "            all_results[node_id]['contributions'].append({\n",
    "                'query': i,\n",
    "                'original': score,\n",
    "                'z_score': z_score,\n",
    "                'normalized': normalized_score\n",
    "            })\n",
    "    \n",
    "    # Step 2: Sort by combined distribution-based score\n",
    "    sorted_results = sorted(\n",
    "        all_results.values(),\n",
    "        key=lambda x: x['combined_score'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCombined Distribution-Based Results (top 3):\")\n",
    "    for i, result in enumerate(sorted_results[:3], 1):\n",
    "        print(f\"{i}. Combined Score: {result['combined_score']:.4f}\")\n",
    "        print(f\"   Statistical breakdown:\")\n",
    "        for contrib in result['contributions']:\n",
    "            print(f\"     Query {contrib['query']}: {contrib['original']:.3f} â†’ \"\n",
    "                  f\"z={contrib['z_score']:.2f} â†’ {contrib['normalized']:.3f}\")\n",
    "        print(f\"   Text: {result['node'].text[:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"Distribution-Based Process:\")\n",
    "    print(\"1. Calculate mean and std for each query variation\")\n",
    "    print(\"2. Z-score normalize: z = (score - mean) / std\")\n",
    "    print(\"3. Sigmoid transform: normalized = 1 / (1 + exp(-z))\")\n",
    "    print(\"4. Sum normalized scores across variations\")\n",
    "    print(\"5. Results reflect statistical significance across all query forms\")\n",
    "\n",
    "# Show fusion mode comparison summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FUSION MODES COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"All three modes tested with the same query for direct comparison:\")\n",
    "print(f\"Query: {query}\")\n",
    "print()\n",
    "print(\"Mode Characteristics:\")\n",
    "print(\"â€¢ RRF (reciprocal_rerank): Most robust, rank-based, scale-invariant\")\n",
    "print(\"â€¢ Relative Score: Preserves confidence, normalizes by max score\")  \n",
    "print(\"â€¢ Distribution-Based: Most sophisticated, statistical normalization\")\n",
    "print()\n",
    "print(\"Choose based on your use case:\")\n",
    "print(\"- Production stability â†’ RRF\")\n",
    "print(\"- Score interpretability â†’ Relative Score\")\n",
    "print(\"- Statistical robustness â†’ Distribution-Based\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
