### 1. `What_is_an_LLM.txt`
```
What is an LLM?

LLM stands for Large Language Model. It is a deep neural network (usually based on the Transformer architecture) trained on massive amounts of text data (hundreds of GB to terabytes) to predict the next token in a sequence. Modern LLMs like Llama, GPT, Mistral, Qwen, Gemma, and Phi can follow instructions, reason step-by-step, write code, translate languages, and answer questions with near-human fluency.
```

### 2. `What_are_Transformers.txt`
```
What are Transformers?

Transformers are a neural network architecture introduced in the 2017 paper "Attention is All You Need" by Vaswani et al. They replaced RNNs and LSTMs by using self-attention mechanisms that process entire sequences in parallel. Transformers dominate NLP, computer vision (ViT), audio, and multimodal AI in 2025.
```

### 3. `Transformer_Architecture.txt`
```
Explain the Transformer model architecture.

A Transformer consists of an encoder and a decoder stack (original design). Each layer contains:
• Multi-Head Self-Attention – allows the model to focus on different parts of the input simultaneously
• Feed-Forward Neural Network – applied position-wise
• Layer Normalization + Residual Connections
• Positional Encoding (since attention has no sense of order)

Modern LLMs (GPT, Llama, Mistral, etc.) use only the decoder part with masked (causal) attention for autoregressive text generation.
```

### 4. `Scaled_Dot_Product_Attention.txt`
```
Define Scaled Dot-Product Attention

Scaled Dot-Product Attention is the core mechanism in Transformers:
Attention(Q, K, V) = softmax( (Q K^T) / √d_k ) × V
where:
- Q = Queries, K = Keys, V = Values
- d_k = dimension of the key vectors
The scaling factor 1/√d_k prevents the dot products from growing too large in high dimensions, avoiding vanishing gradients in softmax.
```

### 5. `What_is_RAG.txt`
```
What is RAG?

RAG (Retrieval-Augmented Generation) is a hybrid approach that combines a retriever (usually a vector database like FAISS, Chroma, or Pinecone) with a generative LLM. Instead of relying only on the LLM's internal knowledge, RAG fetches relevant external documents at query time and injects them into the prompt — dramatically reducing hallucinations and enabling up-to-date or domain-specific answers.
```

### 6. `Stages_in_RAG.txt`
```
Explain the different stages in RAG

1. Indexing (Offline):
   • Load documents (PDFs, web pages, etc.)
   • Chunk text into manageable pieces
   • Generate embeddings using an embedding model
   • Store embeddings + metadata in a vector database

2. Inference (Online):
   • User asks a question
   • Question is embedded
   • Retrieve top-k most similar chunks
   • Insert chunks into the LLM prompt
   • LLM generates the final answer using only the provided context
```

### 7. `Best_Small_LLM_CPU_8GB.txt`
```
What’s the best small LLM for CPU-only RAG under 8 GB RAM?

In 2025, the clear winners are:
1. Microsoft Phi-3.5-mini-instruct (3.8B) – Q4_K_M (~2.4 GB RAM) → often beats 7B/8B models
2. Qwen2.5-3B-Instruct – best reasoning on CPU
3. Google Gemma-2-2B-it – fastest inference (40–50 t/s)
4. Meta Llama-3.2-3B-Instruct – official, very reliable

All run smoothly at 25–50 tokens/sec on a modern laptop with 8 GB RAM.
```

### 8. `What_is_Chunking.txt`
```
What is chunking?

Chunking is the process of splitting large documents into smaller, semantically meaningful pieces (typically 256–1024 tokens) before embedding them. Proper chunking ensures that retrieved context contains complete ideas, not broken sentences, which is critical for accurate RAG performance.
```

### 9. `Best_Chunk_Size_for_PDFs.txt`
```
What is the best chunk size and overlap for PDFs?

2025 best practices for PDFs:
• Chunk size: 512–1024 tokens (~800–1500 characters)
• Overlap: 100–200 tokens (20–30% overlap)

Smaller chunks (≤256) hurt complex reasoning. Larger chunks risk truncation.
Advanced: Use semantic/proposition-based chunking (via LLM or tools like LlamaIndex) — improves accuracy by 10–25% over fixed-size chunking.
```

### 10. `What_is_Hallucination.txt`
```
What is hallucination?

Hallucination occurs when an LLM generates confident but factually incorrect or completely fabricated information. It happens because LLMs are trained to produce plausible text, not to verify truth against external sources.
```

### 11. `Fix_RAG_Hallucination.txt`
```
My RAG is hallucinating even with context. How do I fix it?

Top 7 fixes (apply in order):
1. Strong system prompt: "Answer using ONLY the provided context. If unsure, say 'I don’t know.'"
2. Put context AFTER the question in the prompt
3. Use 2–4 high-quality chunks instead of 8–10 noisy ones
4. Force citation: "Cite sources using [Source 1], [Source 2]"
5. Switch to a more obedient model (Phi-3.5, Qwen2.5, Llama-3.2/3.3)
6. Add HyDE or multi-query retrieval
7. Add a self-check step (LLM judges its own answer for faithfulness)
```

### 12. `Best_Embedding_Model_2025.txt`
```
Which embedding model should I use in 2025?

Top 3 in 2025:
1. BAAI/bge-m3 (multilingual + multi-functionality, supports dense, sparse, and colbert)
2. BAAI/bge-large-en-v1.5 – best pure English performance
3. Snowflake/snowflake-arctic-embed-l – excellent speed/quality trade-off

All significantly outperform all-MiniLM-L6-v2 and older models on MTEB leaderboards.
```

### 13. `Evaluate_RAG_System.txt`
```
How do I evaluate my RAG system properly?

Use these standard metrics (via RAGAS, ARES, or DeepEval):
• Answer Correctness (factual accuracy)
• Faithfulness (does answer stick to retrieved context?)
• Context Relevance
• Answer Relevance
• Context Recall & Precision

Run on a golden dataset of 50–200 question-answer pairs from your domain.
```

### 14. `Update_RAG_Knowledge_Base.txt`
```
How do I update the knowledge base without rebuilding the entire index?

Use incremental indexing:
• FAISS: Use IndexIVFFlat or IndexHNSW + add() method
• Chroma / Pinecone / Weaviate / Qdrant: all support upsert (update + insert)
• Track document hashes or timestamps to detect changes
• On new/changed docs: embed only the new chunks and add them

Never rebuild from scratch in production.
